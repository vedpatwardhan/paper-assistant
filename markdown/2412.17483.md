## A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression

Chenlong Deng 1 , 2 † , Zhisong Zhang 2 ∗ , Kelong Mao 1 , Shuaiyi Li 2 , Xinting Huang 2 , Dong Yu 2 , Zhicheng Dou 1 ∗ 1 Gaoling School of Artificial Intelligence, Renmin University of China 2 Tencent AI Lab {dengchenlong,dou}@ruc.edu.cn zhisonzhang@tencent.com

## Abstract

In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gistbased compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segmentwise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.

## 1 Introduction

Large language models (LLMs) are increasingly recognized as a key pathway toward general artificial intelligence (OpenAI, 2023; Zhao et al., 2023), with long-context processing emerging as a critical research frontier (Chen et al., 2023; Peng et al., 2024). This capability is crucial for advanced applications like retrieval-augmented generation (RAG), long-term memory systems, and complex reasoning frameworks (Gao et al., 2023; Zhu et al., 2023; Zhang et al., 2024c; Wei et al., 2022; Lightman et al., 2024). Despite the proliferation of architectural innovations, Transformerbased models remain the performance standard.

However, these architectures face significant computational challenges when processing extended text sequences: the key-value (KV) cache memory grows linearly with sequence length, while the attention mechanism's quadratic computational scaling introduces substantial overhead. In models like Llama3-8B (Meta-Llama, 2024), a 128K context KV cache can consume memory equivalent to the entire model's parameters, limiting deployment on edge devices and constraining context windows.

A promising approach to mitigate these challenges involves reducing overhead by compressing the number of past tokens stored in the KV cache. This work focuses on a specific type of compression method that condenses the context into a small set of special tokens, called gist tokens (Mu et al., 2023). 1 By replacing the original tokens with a limited number of gist tokens, these methods effectively reduce both KV cache size and computational cost. While such techniques have been successfully applied in real-world tasks (Qian et al., 2024), two critical questions remain unresolved:

Q1 : To what extent can this architecture replace full attention models? Q2 : Does the compression introduce potential, yet significant, failure patterns?

In this work, we thoroughly investigate these two questions through extensive experiments. Specifically, we propose a unified framework for categorizing existing gist-based model architectures along two dimensions: Memory Location and Gist Granularity . We provide comprehensive evaluations for them with a wide range of language tasks.

For Q1 , our findings indicate that the finegrained KV cache architecture (referred to as Fine KV ) is highly effective, achieving near-lossless compression performance on various tasks, such as RAG, long-document QA, and summarization, when compared to the full attention model. How-

Figure 1: Overview of gist token-based context compression architectures. Long texts are segmented for compression, enabling diverse architectures through different memory locations and gist granularity .

<!-- image -->

ever, it still exhibits notable gaps in tasks like reranking and synthetic recall, suggesting that while promising, it is prone to severe compression failures in certain scenarios. Regarding Q2 , we conduct a probing experiment focused on context reconstruction and discover that the compression bottlenecks occur in the gist representations. We further identify three failure patterns resulting from this bottleneck: 1) lost by the boundary , where generation degrades near the start of a segment; 2) lost if surprise , where unexpected details tend to be ignored if budgets are limited; and 3) lost along the way , where compressed models make errors midway for tasks requiring precise recall.

Building on the above findings, we further propose two strategies to enhance the Fine KV architecture for more effective context compression. The first, fine-grained autoencoding , adds a weak decoder with an autoencoding loss to reconstruct original token information from gist tokens, ensuring efficient and accurate compression. The second, segment-wise token importance estimation , adjusts loss weights based on a token's dependency on the compressed context, dynamically optimizing tokens that require more contextual understanding. Experiments show that both strategies significantly improve model performance, with joint optimization achieving the best results.

The contributions of this work are:

- · We propose a unified framework for categorizing existing gist-based model architectures and conduct comprehensive experiments to evaluate their effectiveness. (§2)
- · We show that that gist-based models achieve nearlossless performance on many tasks but still face challenges in particular scenarios. (§3)
- · We identify three critical failure patterns arising from compression bottlenecks, offering valuable insights into the limitations of current gist-based compression methods. (§4)
- · We propose two strategies: fine-grained autoencoding and segment-wise token importance estimation, which effectively mitigate these bottlenecks and enhance model performance. (§5)

## 2 Preliminaries

Gist token-based context compression reduces KV cache by using some special tokens, which are referred to as gists, to represent the full context. The number of special tokens is much fewer than that of the full context, leading to lower memory usage. While many pervious work studies compressing the full prompt at once (Mu et al., 2023; Ge et al., 2024b), we focus on a generalized scenario that dynamically compresses and generates context on the fly, as such setting holds promise for broader general-purpose tasks. To this end, we provide a unified perspective to analyze and understand existing architectures.

Figure 1 illustrates an overview of gist-based context compression methods. We take a segmentwise approach that splits the input sequence into segments and iteratively applies compression for each segment. Assuming an input sequence X = [ x 1 , . . . , x n ] , it is divided into segments of fixed length L , where the i -th segment is represented as S i = [ x ( i -1) · L +1 , . . . , x ( i -1) · L + L ] . When processing the i -th segment, the model accumulates all previously compressed information and generates new compressed representations as the memory for

later processing:

ˆ G < ( i +1) ← LLM ([ ˆ G <i , Insert ( S i , G i )])

Here, G i = [ g 1 , . . . , g t ] are new gist tokens inserted into the i -th segment, and ˆ G i are compressed context representations preceding this segment. The function Insert ( · ) denotes the insertion of gist tokens into the input sequence. This procedure effectively compresses the information of L tokens into t tokens, achieving a compression ratio of L/t . For example, with a compression ratio of 4, every four raw tokens can be replaced by one gist token on average, thereby reaching a 75% reduction in memory usage. Following this formula, existing architectures can be categorized along two dimensions: 'memory location' and 'gist granularity'.

Memory Location After the forward pass of each segment, we can choose to store either the last hidden states of the gist tokens or their KV cache as memory. Opting for the last hidden states is commonly referred to as 'recurrent memory', which serves as input embeddings to deliver compressed context to subsequent segments. Note that this design can be viewed as a segment-wise RNN, and typical representatives include RMT (Bulatov et al., 2022) and AutoCompressors (Chevalier et al., 2023). Alternatively, the KV cache of the gist tokens can be directly reused as the memory to avoid extra computations, and this shares the same design as in sparse attention. Typical representatives of the KV approach include Gist (Mu et al., 2023), Landmark (Mohtashami and Jaggi, 2023) and Activation Beacon (Zhang et al., 2024a).

Gist Granularity The Insert ( · ) function in the formula can be implemented in two ways: (1) Coarse-grained: Gist tokens are appended after all raw tokens, allowing each gist token to attend to the entire segment and all preceding contexts, which is the scheme adopted in most previous works; (2) Fine-grained: Gist tokens are evenly inserted among the raw tokens, enabling each gist token to focus on a specific context, which is investigated in Activation Beacon (Zhang et al., 2024a). Besides, this design can also enhance language modeling through an implicit chain-of-thought mechanism.

Notably, the combination of recurrent memory and fine-grained gist tokens is practically infeasible, since it requires too many non-parallelizable forward passes within a segment. Therefore, we mainly explore the remaining three combinations in this work, as illustrated in Figure 1.

## 3 Can Gist Tokens Replace Full Attention in an Efficient and Effective Way?

## 3.1 Experimental Setup

Training Recipes In our main experiments, we perform continued-training on the base models using a general-purpose corpus to analyze their intrinsic context compression capabilities. To avoid potential confounding effects from techniques like supervised fine-tuning, we focus exclusively on the base models rather than the SFT ones. 2 Specifically, we select Llama3.1-8B (Meta-Llama, 2024) and Qwen2-7B (Qwen-Team, 2024) as our base models, given their widespread recognition and adoption in the community. We use the SlimPajama dataset and follow the processing procedure of Fu et al. (2024), by upsampling long sequences and ultimately obtaining 3B tokens for training. Further training details are provided in Appendix A.

Evaluation Tasks We perform extensive experiments, covering a wide range of tasks: (1) Language modeling , for which we evaluate perplexity on PG19 (Rae et al., 2020), ProofPile (Zhangir Azerbayev), and CodeParrot (CodeParrot); (2) Weak Context-dependent Tasks , 3 for which we evaluate four tasks with MMLUPro (Wang et al., 2024), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), and BBH (Suzgun et al., 2023), to evaluate the model's abilities in knowledge, mathematics, common sense, and comprehensive reasoning, respectively; (3) Long Context Tasks , which thoroughly assess the model's handling of long texts and we select seven types of tasks: RAG, Rerank, LongQA, Many-shot ICL, Synthetic Recall, Summarization, and Code. The datasets selected for testing these tasks include portions from popular longtext benchmarks such as RULER (Hsieh et al., 2024) and ∞ Bench (Zhang et al., 2024b). Inspired by Yen et al. (2024)'s setting, we adopt 2-shot demonstrations to ensure a robust evaluation of long-context performance. Further details on the datasets and metrics are provided in Appendix B.

## 3.2 Overall Performance Comparisons

We present the results of the Llama model in the main text, while the results of the Qwen model are

Figure 2: Comparisons of different compression methods on perplexity evaluation for language modeling.

<!-- image -->

presented in Appendix C.

Language Modeling As shown in Figure 2, the differences between the architectures are clear and consistent across all datasets. Full attention outperforms all methods that compress contexts. Among the compression-enhanced architectures, fine-grained compression delivers better performance than coarse-grained, and KV cache performs better than recurrent memory. Note that the absolute differences in perplexity are small; for example, with a compression ratio of 4, the gap between the fine-grained KV cache and the full attention on Proof-Pile is only 0.1.

Weak Context-dependent Tasks As shown in Table 1, 4 among four datasets, full attention shows a clear advantage only on the BBH dataset, which involves some complex reasoning tasks. In the BBH dataset, reasoning paths can usually extend over several hundred tokens. Long-form reasoning within compressed contexts frequently encounters challenges, such as generating content that spans multiple segments, which results in the accumulation of substantial inaccuracies during the process. This severely impacts the final output. However, in the other three datasets, despite the diversity of task types, the reasoning paths are typically only dozens of tokens long, which explains why compression models maintain near-lossless performance.

Long Context Tasks Table 2 presents the results, where we have the following findings: (1) Higher Compression Ratio Leads to Lower Performance. While Fine-KV can achieve comparable performance to full attention in some tasks at lower compression ratios (e.g., 4), it struggle to maintain this level of performance at higher ratios. (2) The extent of performance degradation in compressed models varies significantly

Table 1: Performance on weak context-dependent tasks.

across different types of tasks. For tasks where the required information is somewhat fuzzy (e.g., Summarization), or where the query is closely related to the general topics of the context (e.g., RAG and LongQA), compression does not noticeably affect the performance. For many-shot ICL, which requires almost the full context, the fine-grained KVcache can maintain performance comparable to full attention even at low compression rates. However, in tasks that demand precise rephrasing or involve highly complex multi-hop reasoning, such as Rerank 5 , none of the compressed models perform on par with full attention. (3) Coarse-grained methods appear to struggle in fully utilizing the available memory budget. Despite having the same memory budget, the Fine-KV's performance decreases systematically as the compression rate increases, whereas coarse-grained methods show consistently poor performance across different ratios. The trends observed in perplexity evaluation support this finding, suggesting that coarse-grained gist placement is less effective at learning how to optimize the memory budget for compression.

Table 2: Performance comparison among full attention and compression architectures on long context tasks. Bold indicates the best result along the same compression ratio.

## 4 Understanding Why and How Compression Fails

Previous results show that gist token-based context compression exhibits a discernible performance gap compared to full attention, particularly in tasks like synthetic recall that require exact rehearsal. This suggests the presence of a 'compression bottleneck' that prevents the language model from treating gist tokens as equivalent to uncompressed context. We conduct a probing experiment to investigate the nature of this bottleneck and examine three critical failure modes arising from it.

## 4.1 Compression Bottleneck Probing

Experimental Setting We adopt the concept of autoencoder to investigate the quality of compressed representations in gist tokens. For this experiment, we use the Fine-KV architecture, which is the most effective compression architecture according to previous results. We evaluate whether each gist token completely stores the contextual information of its corresponding snippet by training a probing decoder to recover the corresponding token sequence. We examine two decoders: an LLAMA38B model that inherits the full pre-trained parameters and a model with only a single transformer block. This allows us to explore the compression quality from the perspective of decoder capacities.

Results In Table 3, we report the training loss after 2K training steps for two models, along with their token-level reconstruction accuracy on the PG19 dataset. Although the full model demonstrates superior performance, it still exhibits significant shortcomings in decoding the information

Table 3: Reconstruction accuracies with different compression ratios (CR).

within gist tokens. Under high compression ratios, the model's accuracy even falls below 20%, indicating that it can only retain fuzzy content rather than remember the precise details from the original context. Ideally, copying a small set of recent tokens should be an easy task, yet probing experiments reveal poor performance. This suggests that the representations of current gist token memory impose a severe compression bottleneck, limiting the model's capacity to extract and utilize contextual information effectively.

## 4.2 Failure Pattern Observations

The compression bottleneck may evolve into specific failure patterns. We highlight three representative and interesting patterns:

Lost by the boundary This discovery stems from an analysis of token-level perplexity distribution. As illustrated in Figure 3, we compute the average perplexity of the tokens at each position within individual segments, excluding the first segment since it lacks gist tokens as contextual input. The results reveal that, while token perplexity in the full attention model remains relatively uniform across positions, the compressed model exhibits a clear pattern of higher perplexity at the start of the segment and lower perplexity toward the end.

Furthermore, we evaluated the impact on generation tasks by truncating the context to a specific

Figure 3: Average Perplexity of tokens in different positions among segments.

<!-- image -->

<!-- image -->

Truncate Context to the Last

k

Tokens

Figure 4: Performance on different tasks while truncating context to the last k tokens. When k is a multiple of 2048, the model will generate near the boundary.Figure 5: Performance on the 32-digit uuid recall task. We report the exact match rates of various firstk digits.

<!-- image -->

length. As shown in Figure 4, with a segment length set to 2K, the performance when generation starts at the beginning of a segment is substantially worse compared to the case when generation starts from the middle of a segment. This indicates that the segment boundary effects influence not only the accuracy of reading specific information but also the model's overall language modeling capability.

Lost if surprise We find that under constrained memory budgets, the model tends to prioritize retaining detailed information that closely aligns with

Table 4: Performance on synthetic recall task (PopQA).

the overarching theme of the context. To validate this, we construct a synthetic dataset 6 with different configurations based on the PopQA dataset from the RAG task, as it provides explicit question subjects, and most documents are typically related to the same subject. We randomly insert a 'needle' between sentences in the gold document, formatted as: '{subj}'s special {needle\_type} is {nee-dle\_content}'. Here, {subj} can either be the original subject or 'Mr. Tree', while {needle\_type} can be either 'food' or an 8-digit number. When {subj} is the original subject, we consider the needle to be relevant to the theme of most of the context; otherwise, it is surprising and unrelated. All needles are transformed into compressed gist tokens during the model's decoding stage. As shown in Table 4, our experimental results reveal significant performance differences in both needle types when altering only the subject of a single sentence. This indicates that the successful retrieval of compressed information is associated with its relevance to the context. An 'unexpected' information is more likely to be lost during compression.

Lost along the way We notice that compressionenhanced architectures struggle to recover exact rehearsal effectively. When dealing with a relatively long 'needle', the compression process can scatter critical information across multiple gist tokens. Consequently, even if the model identifies the beginning of the target information, it risks losing track during subsequent steps of generation. To validate this observation, we conducted a re-

call experiment using 32-digit UUIDs, comparing the performance of full attention models against compressed models, and analyzed their accuracy across prefixes of varying lengths. As illustrated in Figure 5, the replication accuracy of full attention models remains stable regardless of prefix length, suggesting that once the starting point is identified, copying the rest of the content is straightforward. In contrast, compressed models show a significant drop in accuracy, decreasing to less than half of the original as the prefix extends from the first four digits to all 32 digits. This finding highlights the reduced copying reliability associated with compressed representations.

## 5 Mitigating Compression Flaws

## 5.1 Methodology

Building on these findings, we have identified critical shortcomings in the current architecture's context compression. In this section, we propose two effective learning strategies to address them.

Fine-grained Autoencoding (AE) The probing experiments in Section 4 indicate that the compressed representations of current gist tokens struggle to reconstruct the original content. To address this issue, we introduce an additional autoencoding loss during training to explicitly encourage the retention of the original contextual information. Different from ICAE (Ge et al., 2024b), we require each gist token to be responsible for a specific snippet. Following the mainstream conclusion in autoencoding research that weak decoders help learn better representations (Lu et al., 2021), we adopt a single-layer transformer as the decoder. For each gist token g kv i , the objective is to reconstruct the original token sequence between the current and previous gist tokens. The input for this task is:

[ g kv i , [ ae ] r , x 1 , . . . , x r ]

where [ ae ] r is a special token to prompt model to reconstruct r tokens (i.e., x 1 to x r ). The loss of autoencoding is similarly defined in an autoregressive way:

L ae = 1 N 1 r N ∑ i =1 r ∑ j =1 log P θ ( x j | g kv i , [ ae ] r , x <j )

Segment-wise Token Importance Estimation (TIE) Another approach to promote compression is to adjust the loss weights of different tokens,

since each token depends on the context in different degrees. We hypothesize that the importance of a token is determined by the modeling difficulty it presents during segment-wise compression. The more a token relies on the compressed gist context for prediction, the more effort should be dedicated to learning it. Inspired by LongPPL (Fang et al., 2024), we estimate the reliance of each token ( x i ) on the gist context and allocate a tailored learning weight w i accordingly:

Diff ( x i ) = min (log P θ ( x i | x seg <i ) P θ ( x i | x full <i ) , γ ) , w i = e Diff ( x i ) ∑ N j =1 e Diff ( x j ) .

Here, P θ denotes the original language model, x seg <i denotes the preceding tokens only in the current segment, and x full <i denotes the full context, including tokens in previous segments. This reliance is quantified by analyzing the difference in modeling probabilities when the token attends to the full context versus the local segment alone.

## 5.2 Experiments

Boundary Effect Test Previous results show that gist-based models demonstrate strong performance on weak context-dependent tasks but are severely constrained by the 'lost by the boundary' phenomenon. We test two improved methods under the same experimental conditions in Section 4, with the results presented in Table 6. Both methods significantly enhance performance in boundary regions, particularly on the BBH dataset, which involves tasks requiring long-form reasoning. This improvement may be attributed to their ability to reduce the accumulation of errors during the generation process. While these methods do not completely eliminate the boundary effect, they offer promising strategies for mitigating its impact.

Long Context Tasks Table 5 highlights that both methods consistently enhance the model's performance on long-context tasks, particularly under low compression ratios. Key observations include: (1) For tasks where the performance gap between the compression-enhanced model and full attention is relatively small (e.g., RAG and LongQA), both methods maintain excellent performance without negative impacts. For the many-shot ICL task, they even demonstrate continuous improvements. (2) For tasks where the original architectures strug-

Table 5: Performance comparisons using our methods, with the best 'average' results bolded for clarity.

Table 6: Improvements of our mitigating methods on the 'lost by the boundary' problem.

gle, such as rerank and synthetic recall, both methods deliver remarkable performance gains. For instance, under a compression ratio of 4, the improvements on the synthetic recall task reach as high as 52.7% and 33.7%, respectively. These indicate that our methods can effectively enhance the model to read context information from gist tokens.

## 6 Related Work

KV Cache Compression Recent work has explored KV cache optimization at the layer , head , token , and tensor levels. Layer -level methods merge caches across layers using inter-layer similarities (Brandon et al., 2024; Sun et al., 2024; Wu and Tu, 2024; Liu et al., 2024a). Head -level techniques allow multiple query heads to share keyvalue pairs (Ainslie et al., 2023; Shazeer, 2019). Tensor -level approaches, such as low-rank approximations, compress caches into compact representations (DeepSeek-AI, 2024), while quantization reduces precision for memory savings (Liu et al., 2024b). Token -level methods preserve only critical tokens, including learnable tokens (Mu et al., 2023; Ge et al., 2024b; Qin and Durme, 2023; Mohtashami and Jaggi, 2023; Chevalier et al., 2023; Zhang et al., 2024a), token eviction (Zhang et al.,

2023; Liu et al., 2023; Ge et al., 2024a), external memory (Xiao et al., 2024a), and hard selection (Li et al., 2023; Jiang et al., 2024b). In this work, we focus on the direction that introduces a few learnable special tokens to replace the previous full context.

Sparse Attention Researchers have been exploring efficient alternatives of full attention (Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Zhou et al., 2022; Tay et al., 2020). Recently, it has been widely observed that LLMs naturally exhibit significant sparse attention patterns, especially in long-form texts (Jiang et al., 2024a). To leverage such characteristics, researchers have developed heuristic or learnable sparsification strategies that achieve significant speedup while maintaining reliable performance (Jiang et al., 2024a; Xiao et al., 2024b). The gist token-based context compression approach can be regarded as a special case of sparse attention with a segment-wise approach (Chevalier et al., 2023; Zhang et al., 2024a): where full attention is employed within each segment.

## 7 Conclusion

Our comprehensive evaluation presents that while gist-based context compression shows promise as an alternative to full attention in many tasks, it still falls short in specific scenarios. Through carefully designed probing experiments, we identify critical compression bottlenecks and typical failure modes. Furthermore, we propose two effective strategies that significantly enhance compression performance. These findings offer new insights and directions for advancing context compression techniques in the future.

## Limitations

Model Scale and Context Length Constrained by our available computational resource, we are able to train long-text large language models with sizes up to 7/8B parameters in a 16K context window. Larger models (e.g., Llama3.1-70B) typically have more layers, which enables them to offer greater memory capacity and stronger reading capabilities under the same compression ratio when using gist token-based compression. Thus, such larger models may offer advantages in reducing performance degradation, but this still needs to be verified in future studies.

Scope of Compression Methods Our study concentrates on a comparative analysis between gist token-based context compression and the full attention mechanism. While other techniques, such as token-dropping methods represented by StreamingLLM and H2O, are also capable of context compression, including them in our scope would go beyond the focus of this paper. Our primary aim is to investigate the effectiveness and limitations of gist token-based context compression, using full attention as the ideal performance upper bound for comparison. Incorporating additional methods would risk complicating the analysis and diluting the focus on the central research question. Therefore, we choose to maintain the scope to ensure clarity and depth in our insights and analysis.

## Ethical Discussion

This study focuses on the performance of gist tokenbased context compression techniques, without introducing explicitly designed features that could directly influence the cognition of language models. We select widely recognized and validated public training datasets. This can minimize the risk of injecting new biases or toxic data. These datasets are typically subjected to rigorous review and curation, ensuring balanced and stable data distributions. As a result, they help mitigate the impact of harmful information on the model's learning process and prevent significant distortions in its cognitive and decision-making patterns.

## References

Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: training generalized multi-query transformer models from multi-head checkpoints. In Pro-

eedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 , pages 4895-4901. Association for Computational Linguistics.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR , abs/2004.05150.

William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan RaganKelley. 2024. Reducing transformer key-value cache size with cross-layer attention. CoRR , abs/2405.12981.

Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. 2022. Recurrent memory transformer. CoRR , abs/2207.06881.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. CoRR , abs/2306.15595.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023 , pages 3829-3846. Association for Computational Linguistics.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR , abs/2110.14168.

CodeParrot. https://huggingface.co/codeparrot/codeparrot.

DeepSeek-AI. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. CoRR , abs/2405.04434.

Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. 2024. What is wrong with perplexity for long-context language modeling? CoRR , abs/2410.23771.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024. Data engineering for scaling language models to 128k context. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net.

Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024. How to train long-context language models (effectively). CoRR , abs/2410.02660.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrievalaugmented generation for large language models: A survey. CoRR , abs/2312.10997.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024a. Model tells you what to discard: Adaptive KV cache compression for llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.

Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024b. In-context autoencoder for context compression in a large language model. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.

Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: what's the real context size of your long-context language models? CoRR , abs/2404.06654.

Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024a. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR , abs/2407.02490.

Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024b. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 , pages 1658-1677. Association for Computational Linguistics.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020 . OpenReview.net.

Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. BOOKSUM: A collection of datasets for long-form narrative summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 6536-6558. Association for Computational Linguistics.

Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing context to enhance inference efficiency of large language models. In Proceedings of

the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 , pages 6342-6353. Association for Computational Linguistics.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Let's verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.

Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. 2024a. Minicache: KV cache compression in depth dimension for large language models. CoRR , abs/2405.14366.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .

Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024b. KIVI: A tuning-free asymmetric 2bit quantization for KV cache. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net.

Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain a strong siamese encoder for dense text retrieval using a weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , pages 2780-2791. Association for Computational Linguistics.

Meta-Llama. 2024. The llama 3 herd of models. CoRR , abs/2407.21783.

Amirkeivan Mohtashami and Martin Jaggi. 2023. Random-access infinite context length for transformers. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .

Jesse Mu, Xiang Li, and Noah D. Goodman. 2023. Learning to compress prompts with gist tokens. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .

OpenAI. 2023. GPT-4 technical report. CoRR , abs/2303.08774.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2024. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.

Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. 2024. Memorag: Moving towards next-gen RAG via memory-inspired knowledge discovery. CoRR , abs/2409.05591.

Guanghui Qin and Benjamin Van Durme. 2023. Nugget: Neural agglomerative embeddings of text. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learning Research , pages 28337-28350. PMLR.

Qwen-Team. 2024. Qwen2 technical report. CoRR , abs/2407.10671.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.

Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. CoRR , abs/1911.02150.

Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. 2024. You only cache once: Decoderdecoder architectures for language models. CoRR , abs/2405.05254.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 13003-13051. Association for Computational Linguistics.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: A survey. CoRR , abs/2009.06732.

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR , abs/2406.01574.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35:

Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 .

Haoyi Wu and Kewei Tu. 2024. Layer-condensed KV cache for efficient inference of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 , pages 11175-11188. Association for Computational Linguistics.

Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. 2024a. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. CoRR , abs/2402.04617.

Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. 2024b. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads. CoRR , abs/2410.10819.

Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. HELMET: how to evaluate longcontext language models effectively and thoroughly. CoRR , abs/2410.02694.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 4791-4800. Association for Computational Linguistics.

Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024a. Long context compression with activation beacon. arXiv preprint arXiv:2401.03462 .

Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b. ∞ bench: Extending long context evaluation beyond 100k tokens. CoRR , abs/2402.13718.

Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024c. A survey on the memory mechanism of large language model based agents. CoRR , abs/2404.13501.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .

Bartosz Piotrowski Zhangir Azerbayev, Edward Ayers. Proofpile: A pre-training dataset of mathematical texts.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR , abs/2303.18223.

Yujia Zhou, Zhicheng Dou, Huaying Yuan, and Zhengyi Ma. 2022. Socialformer: Social network inspired long document modeling for document ranking. In WWW'22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 , pages 339347. ACM.

Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. CoRR , abs/2308.07107.

## A Training Details

We train all models using 2B tokens from the upsampled SlimPajama dataset, with document boundaries marked by the eos token. Each model was augmented with 4 sink tokens to enhance modeling stability. To support dynamic compression ratio assignment, the compression ratio for each data instance is randomly sampled from {4, 8, 16, 32}. The context length of the training data is set to 16K, with a fixed segment length of 2K. The learning rate is set to 1e-5, using a cosine lr scheduler that reduces the learning rate to 50% of its highest value in the end. Additionally, the first 1% of training steps are allocated for learning rate warmup.

## B Evaluation Details

Perplexity The average perplexity is calculated across all data using a 16K-length context window, with a sliding window stride equal to the length of the context window.

Weak Context-dependent Tasks To ensure that the context for each task is compressed at least once, few-shot examples are used to fill the context. The number of examples used for each task is detailed in Table 7. For all tasks except HellaSwag, which selects answers based on the likelihood of candidate answers, the Chain-of-Thought (CoT) reasoning approach is employed to generate answers.

Table 7: Evaluation setting of weak context-dependent tasks.

Long Context Tasks The majority of our task configurations are based on Yen et al. (2024) and Gao et al. (2024), with code tasks leveraging RepoBench. We sample up to 1K samples for each dataset, and contexts are constructed under the configs of a max length of 16K. Details are presented in Table 8. We apply greedy decoding to all generation tasks for stability.

Table 8: Details of long context tasks.

Table 9: Performance of short context tasks.

## B.1 Results in the Short Context Setting

We report model performance in the short context setting in Table 9, in which 2-shot demos are applied and contexts are not compressed. The results indicate that short-context capabilities are not affected by learning compression.

## C Performance of Qwen2-7B

In addition to LLAMA3.1-8B, we also conduct a full set of experiments on another widely acknowledged model, QWEN2-7B. The results are shown in Table 10.

## D Results of Supervised Fine-tuning

Supervised Fine-tuning (SFT) is a critical factor influencing model performance on downstream tasks. Gist token-based context compression models often struggle with certain tasks (e.g., synthetic ones), which may be attributed to the low proportion of long-dependency data in the generalpurpose continue-training corpus. To investigate the effect of high-quality SFT data on the model's compression ability, we fine-tune the LLAMA3.18B-INSTRUCT with the Fine-KV architecture. The training data is consisted with LongAlpaca (Chen

Table 10: Long context performance based on QWEN2-7B.

Table 11: Performance of the compression model after SFT (compression ratio=4).

et al., 2024), BookSum (Kryscinski et al., 2022), and synthetic data from (Zhang et al., 2024a). We then evaluate its performance on long-context tasks. Table 11 presents the detailed results: the finetuned model shows significant gains in the previously weakest task (i.e., synthetic recall), while maintaining its performance on tasks where it already excelled. This suggests that long-range supervised signals effectively enhance the ability of gist tokens to preserve precise information in dense memory. Thus, high-quality SFT data containing long-distance dependencies is not only beneficial but potentially essential for the compression model.

## E Extrapolation Capabilities

This work explores a segment-wise context compression method that can effectively reduce the maximum length that each transformer block needs to model. For example, taking LLAMA3-8B as an example, assuming a fixed compression ratio of 4 and a segment length of 1K, the context length after continue-training would be the same as the pre-training length, which is 8K. Even if the user's input context length reaches 16K, exceeding the maximum length after continue-training, the actual maximum length that each transformer block needs to model would only be (16K-1K)/4+1K=4.75K, which still falls within the pre-trained context length of the model. Since the model has already

Table 12: Performance of compression models when inference length exceeds training length.

learned the corresponding positional encodings during pre-training, this method holds promise for extrapolating actual inference lengths.

Using LLAMA3.1-8B as the base model, we evaluate the compressed model trained with 16K contexts on tasks involving 32K contexts. As shown in Table 12, the results indicate that the compressed model continues to perform well even with context lengths multiple times longer than the training length. This suggests that the ability to read context from gist tokens is generalizable.

## A Synthetic Example in PopQA