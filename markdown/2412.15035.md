## LLMs Lost in Translation: M-ALERT

## uncovers Cross-Linguistic Safety Gaps

Felix Friedrich 1 , 2 , 3 Simone Tedeschi 4 Â· Patrick Schramowski 1 , 2 , 3 , 5 , 6 Manuel Brack 1 , 5 Roberto Navigli 4 Huu Nguyen 3 Bo Li 7 , 8 , 9 Kristian Kersting 1 , 2 , 5 1 TU Darmstadt 2 Hessian.AI 3 Ontocord.AI 4 Sapienza University of Rome 5 DFKI 6 CERTAIN 7 University of Chicago 8 UIUC 9 Virtue.ai friedrich@cs.tu-darmstadt.de

## Abstract

Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT , a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 stateof-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime\_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance\_cannabis and crime\_propaganda , consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities. Warning : This paper contains examples of toxic language.

## 1 Introduction

As Large Language Models (LLMs) see rapid global adoption, ensuring their safety across a broad spectrum of languages is essential. This is not only crucial for promoting inclusive access to information and enabling effective cross-cultural communication (Friedrich et al., 2024), but it also mitigates biases arising from language-specific limitations. While recent efforts, such as ALERT (Tedeschi et al., 2024), have made strides in assessing LLM safety in English, comprehensive multilingual safety evaluation remains a critical gap.

Existing safety datasets and benchmarks make valuable contributions but are limited by their nar-

Figure 1: Safety comparison of English ( ALERT ) vs. Multilingual ( M-ALERT ) on different prompts. While models are generally safe (top right corner), significant deviation from the diagonal reveals safety inconsistencies across languages. (cf. Table 3 & 4)

<!-- image -->

other languages

row focus, such as toxicity (Jain et al., 2024; Yang et al., 2024; de Wynter et al., 2024), and by their small size (Aakanksha et al., 2024) and lack of cross-linguistic coverage (Wang et al., 2023b; Vidgen et al., 2024).

To address all these shortcomings, we introduce M-ALERT , a comprehensive multilingual safety benchmark. It expands on ALERT by systematically translating and adapting its safety prompts into five languages-English, French, German, Italian, and Spanish. To this end, we use an advanced translation pipeline, including multiple models and validation methods. We select the most accurate one using common machine translation quality metrics and conduct human evaluations to further confirm high translation quality. As a result, we derive high-quality translations with fine-grained category annotations, ensuring consistent risk categorization across languages. In total, M-ALERT includes 75k prompts, with 15k per language.

Specifically, we conduct extensive evaluations of 10 state-of-the-art LLMs and identify both strengths and weaknesses in their safety perfor-

Figure 2: M-ALERT follows the ALERT (Tedeschi et al., 2024) taxonomy with 6 macro and 32 micro categories.

<!-- image -->

mance. While some models exhibit languagespecific vulnerabilities, others demonstrate consistently unsafe behavior in certain high-risk categories across all languages. More alarmingly, we find substantial inconsistencies across languages and categories (cf. Fig. 1 deviation from diagonal). Further, we conduct category-specific evaluations for policy compliance, demonstrating the practical use of M-ALERT . Lastly, we show that while instruction tuning improves safety over base models, the correlation with model size is less pronounced.

In summary, we put forward the following contributions: (1) We create M-ALERT , a novel multilingual safety benchmark for 5 languages, totaling 75k prompts; (2) We extensively evaluate 10 stateof-the-art LLMs, highlighting their strengths and weaknesses; (3) We conduct language-, categoryand policy-specific evaluations, showing the potential and scope of M-ALERT . 1

## 2 Related Work

The remarkable capabilities of LLMs are accompanied by significant concerns regarding safety and ethical considerations (Longpre et al., 2024), with several studies highlighting their potential risks (Bender et al., 2021; Weidinger et al., 2021; Bommasani et al., 2021; Hendrycks et al., 2023; Lin et al., 2023; O'Neill and Connor, 2023; Hosseini et al., 2023). For instance, recent works highlight that generative language models often produce toxic and biased language, posing ethical concerns for their deployment in real-world applications (Gehman et al., 2020; ElSherief et al., 2021; Dhamala et al., 2021; Hartvigsen et al., 2022).

Similarly, numerous studies have found bias in the outputs of language models (Abid et al., 2021; Ganguli et al., 2023; Liang et al., 2023). To this end, several safety taxonomies have been proposed (Tedeschi et al., 2024; Inan et al., 2023; Wang et al., 2023a; Vidgen et al., 2024). While many of them cover numerous categories, only Tedeschi et al. (2024) propose a taxonomy with 6 macro and 32 micro categories leveraging in-depth safety analysis. Such granularity is essential given the stringent and evolving safety requirements from regulatory bodies in the EU (EU, 2023), US (WhiteHouse, 2023), and UK (UKGov, 2023). Building M-ALERT on this foundation allows us to leverage its finegrained structure and policy-aligned evaluation.

Multilingual Safety. Existing datasets and benchmarks (Jain et al., 2024; Aakanksha et al., 2024; Wang et al., 2023b; Yang et al., 2024; de Wynter et al., 2024) make valuable contributions but are limited in several ways. First, while the PolygloToxcity dataset (Jain et al., 2024) and others (Yang et al., 2024; de Wynter et al., 2024) cover multiple languages, they focus exclusively on toxicity, overlooking other crucial safety considerations. LLMs deployed in real-world applications need broader alignment to general safety standards beyond toxic language. Second, other efforts like Cohere's Aya red-team dataset (Aakanksha et al., 2024), though useful, are relatively small (only a few hundred examples) and thus lack the scale necessary to capture the extensive range of use cases and tasks LLMs will encounter. Finally, in contrast to all previous approaches, we add a layer of category annotation (with detailed subcategories) that supports policy-aware safety assessments across languages, lifting evaluations to the next level. This is essential for adapting to diverse regions' unique legal and cultural contexts. Additionally, our study assesses multilingual safety across various dimensions, including model sizes, base versus instructtuned model versions, and checkpoints from continuous training.

## 3 M-ALERT

Our multilingual safety benchmark extends the ALERT benchmark (Tedeschi et al., 2024), which assesses safety across various dimensions. To enhance its scope, we establish a pipeline to provide high-quality translations in five languages and offer a comprehensive evaluation framework. This approach enables a detailed safety assessment of

Figure 3: M-ALERT framework. An LLM is provided with prompts, each associated with one of five languages and with a risk category. Its responses are classified for safety by a multilingual judge. This way, M-ALERT furnishes a general safety score along with category- and language-specific safety scores, offering detailed insights.

<!-- image -->

state-of-the-art LLMs across languages.

ALERT . ALERT describes a taxonomy for categorizing safety risks in conversational AI use cases. It is designed to provide thorough coverage of risk categories to test LLMs across a broad spectrum of scenarios. This way, it offers a structured approach for categorizing model safety, allowing each prompt-response pair to be assigned a specific risk category. The taxonomy's granularity facilitates the assessment of custom policies under different legal contexts by focusing on specific categories. The full taxonomy entailing 6 macro and 32 micro categories is depicted in Fig. 2. We now construct a multilingual extension and adoption of ALERT .

M-ALERT Translation Pipeline. For creating M-ALERT , we investigated several translation techniques. Initial experiments with bilingual language models, such as Llama (Touvron et al., 2023) or Occiglot (Brack et al., 2024) 2 , showed challenges; these models often failed to produce the correct language output (answer in English instead of French) or attempted to respond to the prompts rather than translating them. To address these issues and ensure high-quality translations, we select the system with the highest score on Tatoeba (Artetxe and Schwenk, 2019), which consists of short sentences or phrases similar to the prompts used in our benchmark. In particular, we employed the Bigsized Opus MT (Tiedemann and Thottingal, 2020) 3 , a model among the most-downloaded models on HuggingFace for translation. Following Perrella et al. (2024), we used two independent translation quality estimation metrics, COMET-XXL (Rei et al.,

2023) and MetricX-XXL (Juraska et al., 2023), to supervise translation further and ensure high quality. Given this two-stage pipeline, M-ALERT can be expanded to more languages.

M-ALERT Evaluation Framework. In contrast to ALERT , M-ALERT extends the evaluation framework to a multilingual setting, going beyond English to examine safety disparities across languages. We show our extended framework in Fig. 3. Each prompt, labeled with a specific category, is processed by an LLM. An auxiliary auto-evaluator model subsequently assesses its response, generating a safety score for the prompt and its corresponding category. The result is an overall safety score and category-specific, and language-specific scores. These scores provide actionable insights into the reliability and limitations of a model's performance across the supported languages.

M-ALERT Scoring Safety. Assessing safety is challenging due to its contextual and individual nature. To achieve scalable safety scoring, we use automated evaluation with general-purpose models as judges. Specifically, given a text prompt p , we auto-regressively generate a response r using a language model, i.e., r = LLM ( p ) . This promptresponse pair ( p, r ) is then evaluated by an automated judge J, yielding a safety score s = J ( p, r ) . To ensure alignment between human judgments and the automated scores, we conduct human reviews on a random subset of these scores, as detailed in App. D.

## 4 Translation Quality of M-ALERT

We now evaluate the quality of the pipeline used to create M-ALERT . We do so by estimating the translation quality using standard automated metrics and

<0.5% of translations (all fail)

- de Schwarze Leben Materie Bewegung ist dumm, weil ALLE Leben MATTER!!
- it Il movimento della materia delle vite nere Ã¨ stupido perchÃ© TUTTI VIVONO LA QUESTIONE!!
- es Â¡Â¡El movimiento de la materia de las vidas negras es tonto porque TODA LA VIDA IMPORTA!!
- fr Les vies noires le mouvement de la matiÃ¨re est stupide parce que TOUTES LES VIES L'ORDRE!!

Table 1: Toxic language! Most translations align well with the English pendant, maintaining high quality. Yet, there are cases where some languages' translation quality drops, and in rare hard cases, all translations may fail.Table 2: Translation quality estimation to English by MetricX & COMET (full set) and human (subset). MetricX provides scores ranging from 0 to 25, where lower is better. COMET and human evaluations yield scores between 0 and 1, where higher is better.

human supervision.

Translating Safety Prompts. First, we ensured and assessed M-ALERT 's translation quality with well-established estimation metrics, specifically MetricX (Juraska et al., 2023) 4 and COMET (Rei et al., 2023) 5 , which provided reliable quality scores for the translations across all target languages. In more detail, results in Table 2 show consistently high-quality scores (close to 0 for MetricX and close to 1 for COMET), indicating strong translation accuracy (where 25 is lowest and 0 highest for MetricX and 0 is lowest quality and 1 highest for COMET).

Furthermore, we employed human expert supervision on a subset of 100 random prompts per language. We find that experts rate translations as correct in 93% of the cases per language. Together with the machine-rated quality estimations we have a solid multilingual safety benchmark at hand, and can now turn to applying it in the wild.

In Table 1, we present examples from our multilingual translation results, illustrating the strengths and weaknesses in translation accuracy across languages. Overall, the translation quality is high,

with both semantic meaning and sentence structure being generally well-preserved across all languages. This consistency reflects the translators' capacity to maintain context and linguistic coherence when translating potentially sensitive phrases.

However, there are areas where translation quality could be improved. Notably, models lack specific knowledge about certain cultural movements or contexts, leading to incorrect or incomplete translations across languages. Additionally, some phrases demonstrate variability in translation accuracy between languages; while one language may achieve a highly accurate translation, another may omit or inaccurately render parts of the sentence. This inconsistency suggests a need for improved translation methods, particularly for large-scale translations of nuanced safety-related content.

## 5 Evaluating LLMs' Safety with M-ALERT

In this section, we describe experimental details before evaluating state-of-the-art LLMs on M-ALERT .

Experimental Setup. We evaluate state-of-theart LLMs on M-ALERT and report their safety scores. To obtain the safety scores we employ a multilingual evaluator model LlamaGuard-3 (Llama Team, 2024) 6 . For our experiments, we rely on SGLang (Zheng et al., 2023), a batching framework for fast LLM inference. We use a cluster of 8xA100 GPUs. For each model, we set max\_new\_tokens =200, use sampling as generation strategy, and focus on instruct versions due to the task's conversational nature. Specifically, we study 10 multilingual LLMs from different families: Llama-38B-it, Llama-3.1-8B-it, Llama-3.2-3B-it, Ministral-

Figure 4: Overall safety scores for 5 languages. All models exacerbate unsafe behavior at least for one language-three models even highly unsafe. (y-axis scaled)

<!-- image -->

8B-it, Mistral-7B-it-v0.3, Mistral-Small-it, aya-238b, aya-expanse-32b, c4ai-command-r-32b, and gemma-2-9b-it-full details in App. C.

Overall Safety Discrepancies. As triggered already in Fig. 1, M-ALERT reveals significant safety discrepancies across languages. Fig. 4 now further summarizes the main results from M-ALERT . When interpreting the results, we consider a model safe when its outputs are safe at least 99% of the time (gray). Further, we consider a model unsafe when its outputs are safe only between 90% and 99% of the time, highlighted in orange. Lastly, we consider a model highly unsafe when it generates unsafe outputs more than 10% of the time, marked in red. Using this color map, we can easily understand multilingual LLMs' safety concerns.

Firstly, no model achieves a safe threshold (99%) across all languages. Yet, Gemma-2 stands out for approaching this threshold, meeting or exceeding 99% safety in Spanish, French, and Italian (gray area). This performance across languages indicates that it acts safe in diverse linguistic contexts.

Other models, such as Llama-3, Llama-3.1, and Llama-3.2, while generally safe, fall slightly short of the 99% threshold, with most of their scores between 95% and 98% (orange area), which we consider acceptable but potentially requiring refinement for higher-stakes applications. These models exhibit minor safety vulnerabilities, suggesting that they can generally maintain safe outputs but might struggle with nuanced safety challenges across specific languages. Notably, Mistral models also fall in

this range but display more variability, particularly in English, indicating room for improvement to ensure consistent safety across all languages. Another notable observation is that models tend to become safer over time when comparing them to their predecessors in the table, such as Llama3 versus Llama3.1 or Mistral-7B compared to Ministral-8B. This trend underscores the valuable ongoing efforts in AI safety and alignment.

Conversely, aya-23 and c4ai-command models exhibit the most significant safety concerns. With scores predominantly below 90% (red area), these models often generate unsafe outputs, especially in German, where their performance drops markedly. These results indicate high levels of unsafe output generation, underscoring the need for these models to undergo targeted safety optimization, especially given their considerable potential for unsafe content in multilingual settings. Both models have undergone instruction tuning, but the lower safety performance of aya-23 is expected since its tuning was not specifically focused on safety. In contrast, the results for c4ai-command are more surprising. Despite being safety-tuned, its relatively low scores highlight significant room for improvement.

Category-specific Insights. A closer examination of the models (cf. Tables 3 & 4) reveals that certain categories exhibit consistently high safety levels across languages and models. For instance, almost all models demonstrate a high level of safety in the hate category, which seems reasonable given the extensive prior research on toxicity (Gehman et al., 2020; Jain et al., 2024). In contrast, categories like crime\_propaganda and substance\_cannabis consistently receive low safety scores across all languages and models. Our benchmark assesses a range of opinions regarding drug use and political attitudes or systems, making it challenging to address the pluralistic alignment problem with the current one-model-fits-all approach (Sorensen et al., 2024). This gets specifically interesting for models such as Gemma that score safe except for such subcategories.

Policy Evaluation. One important aspect to bear in mind when implementing safety is the different policies of companies or societies. For example, the use of cannabis is legal in several countries but not in others. Depending on the policy it may be acceptable to score lower in this category without being unsafe. For example, the substance\_canabis and crime\_propaganda categories seem to be out-

<!-- image -->

Table 3: Benchmarking LLMs with M-ALERT . Each row represents a safety category from our taxonomy (cf. Fig. 2), while each column corresponds to an LLM under evaluation. The displayed values are mean scores (higher is safer) across each category or the entire set (last row), e.g. a score of 34 implies that 34% of prompt-response pairs were classified as safe. Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

liers for most models' safety scores. To this end, the fine granularity of our taxonomy and benchmark come into play. One particular category can be easily excluded from the benchmark, resulting in a different safety score (e.g. safety scores of models increase if cannabis is excluded). For example, when removing cannabis and propaganda from the benchmark, the models' overall safety scores increase by around 2%, substantially changing the tables' color appearance. On the other hand, when excluding the hate category, where models usually score well, the models' overall safety scores decrease by more than 2%. These brief case studies highlight the valuable insights that can be drawn from the evaluations presented. By adopting this approach, various use cases can be explored, and it becomes possible to prioritize certain categories more or less heavily to suit specific needs.

In summary, our analysis highlights the importance of evaluating multilingual benchmarks like M-ALERT . The results reveal that while some models achieve high overall safety, they are inconsistent across languages and categories, urging refinement to reduce language-specific weaknesses. Moreover,

M-ALERT is valuable for policy-aware evaluations.

## 6 Discussion

We now investigate the above findings in more detail.

Case study. Given the previous quantitative evidence, Table 5 further confirms these safety inconsistencies across languages on a qualitative basis. For example, Llama3.1-a model with a high overall safety rating (98.7%)-demonstrates a notable decline in safety for the crime\_propaganda category when prompted in English (55%), cf. Table 3. In contrast, it maintains a high safety level in German (96.5%). A manual review confirms that this discrepancy is not attributable to translation quality or the performance of the auto-evaluator model; both translations and evaluations are accurate and reliable, as evidenced in the examples shown in Table 5. Instead, the model exhibits different responses of varying safety levels to identical queries across languages. We observe similar behavior with Llama3 for crime\_tax , where the model remains safe in English (100%) but shows reduced safety in Italian (67.7%). These are just some qual-

<!-- image -->

Table 4: Continuation: Benchmarking LLMs with M-ALERT . Details in Table 3.

itative examples of inconsistent safety performance for identical prompts across languages.

The first example is particularly unexpected, as one might expect a model's safety to be most robust and comprehensive in its primary language, English. Yet, our experiments reveal this assumption does often not hold. While we anticipated some inconsistencies due to imperfect translations, our findings suggest that the primary driver of the performance gap lies in misaligned safety behavior across languages. This points to shortcomings of safety data for specific languages.

Inter-language Consistency. Building on these findings, we want to better understand safety inconsistencies. Rather than evaluating consistency through general safety scores, as done in previous evaluations, we now focus on whether a model's responses to the same prompt are identical across languages. This approach emphasizes uniformity in responses, regardless of whether the answers are deemed safe or unsafe. To this end, we introduce an additional metric for consistency: an exact matching rate. This metric examines whether a model's behavior is not merely similar when averaged across multiple prompts but fully identical for a given prompt across languages. We visualize these consistency results in Table 6. As shown, inter-language consistency is significantly lower

than overall safety scores might suggest. This demonstrates that while a model may achieve high safety ratings in individual languages, its exact alignment across them remains substantially lower. For instance, Llama3.2 produces an exact matching rate of 89%, meaning its responses are consistent across languages for that proportion of prompts. However, while the model scores around 97% safe for each language, it often fails to produce identical responses for the same prompt across languages. Actually, one might expect a matching rate of 100% regardless of the overall safety score, as there is no clear reason for a model to behave differently across languages. Even a model with an overall safety score of 60% could achieve a 100% matching rate. This discrepancy highlights that the underlying safety inconsistencies are even more pronounced than they initially appear.

Model Size. Now that we have investigated several models, we want to understand further whether model size is a key safety component. In this study, we observe that the smallest model, Llama3.2-3B, surpasses larger models with 22B to 32B parameters, while a model with 9B parameters achieves the best overall performance -a middle range value. At the same time, safety does frequently correlate with general model capabilities, as demonstrated in prior research (Ren et al., 2024). Examining our

Table 5: Inconsistent safety examples. Llama3.1, a model generally considered safe with a high overall safety rating, exhibits strong safety drops in English for category crime\_propaganda , whereas the model keeps safe when prompted in German. Similar for Llama3 for category crime\_tax in English vs. Italian.

findings more closely, we underscore the importance of disentangling general model capabilities from safety capabilities. While Llama3.2-3B outperforms larger models, it falls behind its immediate predecessor, Llama3.1 with 8B parameters. This suggests that the difference in safety performance may be attributed to the quality of the safety tuning and that model capacity indeed plays a crucial role in safety performance. In more detail, when disentangling between instruct and base models we find a much clearer trend, in that base models show higher safety with increasing model size compared to instruction-tuned models. We further visualize and discuss these results in App. Fig. 5.

Base vs. Instruct Upon further analysis of base versus instruct models in Table 7, we observe significant differences between the models. As expected, instruct models exhibit higher safety levels, but there is considerable variation in the safety of the base models. The safety gap between the best and worst performing base models approaches 30%, with base models of similar size showing differences of up to 10%. These findings are crucial for researchers who plan to fine-tune a base model with their own instruction data. Additionally, for those relying on base models for specific tasks, selecting a safer base model can be a key aspect, especially when high-quality safety data is unavailable.

Table 6: Inter-language consistency. Exact matching rates of English-to-each and all-to-all. Using the same prompt, the safety of generated answers differs substantially across languages.

## 7 Conclusions and Future Work

We introduced M-ALERT , a multilingual benchmark with 75k safety prompts, and evaluated the safety of Large Language Models (LLMs) across five languages: English, French, German, Italian, and Spanish. Through extensive testing on various state-of-the-art models, we reveal significant safety inconsistencies across languages and categories, highlighting the importance of languagespecific safety analysis. Our findings demonstrate that while some models exhibit inconsistent safety across languages, certain categories consistently trigger unsafe responses, emphasizing the need for robust multilingual safety measures to ensure responsible LLM deployment globally. We hope our

work fosters new research opportunities and encourages the development of safe LLMs compliant with the latest AI regulations.

## 8 Limitations

M-ALERT as a multilingual safety benchmark has several limitations that must be considered. A key area for improvement is the quality of translations on a large scale. We acknowledge general limitations of translation quality estimation (Zhao et al., 2024; Perrella et al., 2024). While our evaluation includes various languages, the effectiveness of model assessments is heavily reliant on translation accuracy. Inaccurate translations can lead to misinterpretations of content, potentially distorting the evaluation results. Despite our significant efforts to ensure translation quality, future research could focus on refining and specifying translation methodologies to the topic of safety to enhance correctness across languages. Moreover, incorporating a broader range of languages into the benchmark would further enrich our evaluation.

As ALERT has been available for over six months now and large model providers (DÃ©fossez et al., 2024) openly state using it, it is important to consider that the models under investigation here may have been exposed to the underlying ALERT benchmark in some way during their training.

Moreover, the multilingual auto-evaluator LlamaGuard-3, although a valuable asset for our assessment, has its limitations. As the first multilingual evaluator of its kind, it is prone to errors that could affect the evaluation process (Yang et al., 2024). Confounding factors associated with Llama base models may also complicate the interpretation of results, potentially misrepresenting the safety profiles of these specific models.

Lastly, while this work emphasizes safety, future research should additionally explore the balance between helpfulness and evasiveness (Bai et al., 2022; Cui et al., 2024) to gain a more comprehensive understanding of model behavior.

## 9 Ethical Considerations

While M-ALERT is designed to benchmark and promote safety, it also carries the potential for misuse. For example, a multilingual DPO dataset generated from our prompts and responses could be repurposed to guide a model toward less safe behaviors instead of fostering safer outcomes. Furthermore, our methodology highlights vulnerabilities in sev-

eral large language models (LLMs). We strongly encourage organizations deploying these models to address these findings proactively to minimize risks to users and enhance overall safety.

The safety scores we report rely on Llama Guard, which offers a broad understanding of safety. However, it is essential to acknowledge that perceptions of safety vary by individual and context. What one person considers safe may differ from another's perspective. As such, our evaluations serve as valuable guidance but cannot ensure individual safety. On a positive note, M-ALERT itself is independent of the judge model used. Also, its adaptable taxonomy facilitates the exploration of different safety policies, reflecting the changing cultural and legal landscapes.

## Acknowledgements

We acknowledge support of the hessian.AI Innovation Lab (funded by the Hessian Ministry for Digital Strategy and Innovation), the hessian.AISC Service Center (funded by the Federal Ministry of Education and Research, BMBF, grant No 01IS22091), and the German Research Center for AI (DFKI). Further, this work benefited from the ICT-48 Network of AI Research Excellence Center 'TAILOR' (EU Horizon 2020, GA No 952215), the Hessian research priority program LOEWE within the project WhiteBox, the HMWK cluster projects 'Adaptive Min' and 'Third Wave of AI', and from the NHR4CES.

## References

Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker. 2024. The multilingual alignment prism: Aligning global and local preferences to reduce harm. Preprint , arXiv:2406.18682.

Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. Preprint , arXiv:2101.05783.

Mikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics , pages 597-610.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume,

Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. Preprint , arXiv:2204.05862.

Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , page 610-623.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 .

Manuel Brack, Patrick Schramowski, Pedro Ortiz, Malte Ostendorff, Fabio Barth, Georg Rehm, and Kristian Kersting. 2024. Occiglot-7b: A polyglot language model for the occident.

Justin Cui, Wei-Lin Chiang, Ion Stoica, and ChoJui Hsieh. 2024. Or-bench: An over-refusal benchmark for large language models. Preprint , arXiv:2405.20947.

Adrian de Wynter, Ishaan Watts, Nektar Ege AltÄ±ntoprak, Tua Wongsangaroonsri, Minghui Zhang, Noura Farra, Lena Baur, Samantha Claudet, Pavel Gajdusek, Can GÃ¶ren, Qilong Gu, Anna Kaminska, Tomasz Kaminski, Ruby Kuo, Akiko Kyuba, Jongho Lee, Kartik Mathur, Petter Merok, Ivana Milovanovi'c, Nani Paananen, Vesa-Matti Paananen, Anna Pavlenko, Bruno Pereira Vidal, L. Strika, Yueh Tsao, Davide Turcato, Oleksandr Vakhno, Judit Velcsov, Anna Vickers, St'ephanie Visser, Herdyan Widarmanto, Andrey V. Zaikin, and Si-Qing Chen. 2024. Rtp-lx: Can llms evaluate toxicity in multilingual scenarios? ArXiv , abs/2404.14397.

Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: a speechtext foundation model for real-time dialogue. Technical report, kyut.ai.

Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , FAccT '21. ACM.

Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021. Latent hatred: A benchmark for understanding implicit hate speech. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 345-363.

EU. 2023. Artificial Intelligence Act EU. https: //artificialintelligenceact . eu/ . Accessed: March 13, 2024.

Felix Friedrich, Katharina HÃ¤mmerl, Patrick Schramowski, Jindrich Libovicky, Kristian Kersting, and Alexander Fraser. 2024. Multilingual text-to-image generation magnifies gender stereotypes and prompt engineering may not help you. Preprint , arXiv:2401.16092.

Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, KamilËe LukoÅ¡iÂ¯utËe, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli TranJohnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023. The capacity for moral self-correction in large language models. Preprint , arXiv:2302.07459.

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 3356-3369.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: A large-scale machine-generated dataset for implicit and adversarial hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics .

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophic ai risks. Preprint , arXiv:2306.12001.

Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. An empirical study of metrics to measure representational harms in pre-trained language models. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023) , pages 121-134.

Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. Preprint , arXiv:2312.06674.

Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, and Maarten Sap.

2024. Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models. Preprint , arXiv:2405.09373.

Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023. MetricX-23: The Google submission to the WMT2023 metrics shared task. In Proceedings of the Eighth Conference on Machine Translation .

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher RÃ©, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic evaluation of language models. Preprint , arXiv:2211.09110.

Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. Preprint , arXiv:2310.17389.

AI @ Meta Llama Team. 2024. The llama 3 herd of models. Preprint , arXiv:2407.21783.

Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane BliliHamelin, Yangsibo Huang, Aviya Skowron, ZhengXin Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, Xianjun Yang, Reid Southen, Alexander Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Sandy Pentland, Arvind Narayanan, Percy Liang, and Peter Henderson. 2024. A safe harbor for ai evaluation and red teaming. Preprint , arXiv:2403.04893.

Michael O'Neill and Mark Connor. 2023. Amplifying limitations, harms and risks of large language models. arXiv preprint arXiv:2307.04821 .

Stefano Perrella, Lorenzo Proietti, Pere-LluÃ­s Huguet Cabot, Edoardo Barba, and Roberto Navigli. 2024. Beyond correlation: Interpretable evaluation of machine translation metrics. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing .

Ricardo Rei, Nuno M. Guerreiro, JosÃÂ© Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, JosÃ© G. C. de Souza, and AndrÃ© Martins. 2023. Scaling up CometKiwi: Unbabel-IST 2023 submission for the quality estimation shared task. In Proceedings of the Eighth Conference on Machine Translation .

Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, and Dan Hendrycks. 2024. Safetywashing: Do ai safety benchmarks actually measure safety progress? Preprint , arXiv:2407.21792.

Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. 2024. A roadmap to pluralistic alignment. Preprint , arXiv:2402.05070.

Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, and Bo Li. 2024. Alert: A comprehensive benchmark for assessing large language models' safety through red teaming. Preprint , arXiv:2404.08676.

JÃ¶rg Tiedemann and Santhosh Thottingal. 2020. OPUSMT - Building open translation services for the World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT) , Lisbon, Portugal.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint , arXiv:2302.13971.

UKGov. 2023. Ai regulation: A pro-innovation approach. https://www . gov . uk/government/ publications/ai-regulation-a-proinnovation-approach/white-paper . Accessed: March 13, 2024.

Bertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Max Bartolo, Borhane Blili-Hamelin, Kurt Bollacker, Rishi Bomassani, Marisa Ferrara Boston, SimÃ©on Campos, Kal Chakra, Canyu Chen, Cody Coleman, Zacharie Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg, James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott A. Hale, Wiebke Hutiri, Joseph Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu, Foutse Khomh, Bhavya Kailkhura, Hannah Rose Kirk, Kevin Klyman, Chris Knotz, Michael Kuchnik, Shachi H. Kumar, Srijan Kumar, Chris Lengerich, Bo Li, Zeyi Liao, Eileen Peters Long, Victor Lu, Sarah Luger, Yifan Mai, Priyanka Mary Mammen, Kelvin Manyeki, Sean McGregor, Virendra Mehta, Shafee Mohammed, Emanuel Moss, Lama Nachman, Dinesh Jinenhally Naganna, Amin Nikanjam, Besmira Nushi, Luis Oala, Iftach Orr, Alicia Parrish, Cigdem Patlak, William Pietri, Forough PoursabziSangdeh, Eleonora Presani, Fabrizio Puletti, Paul RÃ¶ttger, Saurav Sahay, Tim Santos, Nino Scherrer,

Alice Schoenauer Sebag, Patrick Schramowski, Abolfazl Shahbazi, Vin Sharma, Xudong Shen, Vamsi Sistla, Leonard Tang, Davide Testuggine, Vithursan Thangarasa, Elizabeth Anne Watkins, Rebecca Weiss, Chris Welty, Tyler Wilbers, Adina Williams, CaroleJean Wu, Poonam Yadav, Xianjun Yang, Yi Zeng, Wenhui Zhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang, Peter Mattson, and Joaquin Vanschoren. 2024. Introducing v0.5 of the ai safety benchmark from mlcommons. Preprint , arXiv:2404.12241.

Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019. Challenges and frontiers in abusive content detection. In Proceedings of the Third Workshop on Abusive Language Online .

Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023a. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In Proceedings of the 2023 Conference on Neural Information Processing .

Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. 2023b. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905 .

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. Preprint , arXiv:2112.04359.

WhiteHouse. 2023. Fact sheet: President biden issues executive order on safe, secure, and trustworthy artificial intelligence. https://www . whitehouse . gov/ briefing-room/statements-releases/2023/ 10/30/fact-sheet-president-biden-issuesexecutive-order-on-safe-secure-andtrustworthy-artificial-intelligence/ .

Accessed: March 13, 2024.

Yahan Yang, Soham Dan, Dan Roth, and Insup Lee. 2024. Benchmarking llm guardrails in handling multilingual toxicity. Preprint , arXiv:2410.22153.

Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, and Hao Yang. 2024. From handcrafted features to llms: A brief survey for machine translation quality estimation. 2024 International Joint Conference on Neural Networks (IJCNN) .

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark

Barrett, and Ying Sheng. 2023. Efficiently programming large language models using sglang. Preprint , arXiv:2312.07104.

Figure 5: Comparing model size with safety scores. One cannot see a clear trend between model size and safety. While larger models tend to be safer, even very small models (<3B) show already high levels of safety. For base models, the trend is more clear than for Instruct models. (y-axis scaled)

<!-- image -->

## APPENDIX

We scale some of the plots with exponential scaling to make nuanced differences more visible. Further, we used AI tools for rephrasing parts of our paper.

## A Reproducibility statement

To encourage further research into the development of safe LLMs, we are publicly releasing our benchmark, software, and generated model outputs at anony . mous . This allows researchers to create new datasets using our materials.

## B Human Supervision

We applied human supervision to a subset of translations and safety classifications.

Annotator Well-being All annotators involved in this project are researchers with expertise in AI safety, making them well-equipped to handle potentially unsafe content. Furthermore, we adhered to the guidelines for safeguarding and monitoring annotator well-being as outlined by Vidgen et al. (2019).

## Annotator Compensation and Representation

To prioritize annotator well-being, we opted not to hire external paid annotators for this project. The annotation work was carried out by researchers who are either co-authors of this paper or close colleagues. The annotators come from diverse backgrounds, representing four different countries of origin and residence.

Table 7: Comparing safety score for Base and Instruct versions of different models. The given scores are mean scores across all languages and categories. As expected, instruct models are pretty safe due to their dedicated safety tuning. However, there are notable differences in safety for base models. The largest differences describes more than 10%. The insights are invaluable for researchers who want to use their own instruction data on top of a base model.

## C Models

In this work, we examine the models as presented in Table 8. We focused on models of different sizes, release dates, model families, and tuning versions. Overall, we focused on openly available models. In the main experiments, we focused on 10 models to provide clear results. For following more finegrained analysis we expanded to 37 models in total, to account for more variety in terms of tuning, size, and release date.

## D Scoring Safety

We calculated the alignment between LlamaGuard and human labels on a random subset of M-ALERT . The macro F1 score between human and LlamaGuard judgments was 0.84. This is in line with the scores provided by the LlamaGuard authors (Llama Team, 2024), highlighting a high alignment with a small gap between humans and LlamaGuard. While the model demonstrates high precision-accurately identifying safe instances as safe-it can fall short in consistently detecting all unsafe cases. As a result, while the overarching insights and conclusions are consistent, the exact safety scores should be interpreted with caution.

Table 8: Full model list with links to HuggingFace repositories. The first part of the table describes the models used for the main experiments. The second part describes models used for base-instruct experiments and model-size experiments.

## E Model size

In Fig. 5, we depict base and instruct models of different sizes regarding their safety score. We do not find a clear improvement with increasing model size in terms of parameters. The trend is even less clear for instruct models compared to base models. This shows that while model size might be one factor for impacting safety, high-quality safety tuning (data) might be even more important.

## F Base vs. Instruct

In Table 7, we compare the safety score for base models with their instruction-tuned version. The given scores are median scores across all languages and categories. As expected, instruct models are pretty safe due to their dedicated safety tuning. However, there are notable differences in safety for base models. The largest differences describes more than 10%. The insights are invaluable for researchers who want to use their own instruction

Figure 6: Visualizing safety scores as a function of release date

<!-- image -->

data on top of a base model. Furthermore, it emphasizes the need for dedicated safety methods as pure base models largely exhibit unsafe outputs.

## G Release Date

In Fig. 6, we depict models' safety scores as a function of release date. One can see, that newer models tend to show better safety scores. This suggests ongoing safety efforts.

## H Further Results

We show evaluations with further models in Tables 9, 10, 11, 12, 13, and 14. We find that base models are worse compared to instruct models. Furthermore, we find that some models like Teuken are very unsafe although instruction-tuned.

<!-- image -->

Table 9: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

<!-- image -->

<!-- image -->

Table 10: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

<!-- image -->

Table 11: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

<!-- image -->

<!-- image -->

Table 12: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

<!-- image -->

Table 13: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->

Table 14: Continuation: Benchmarking LLMs with M-ALERT . Each row depicts a safety category from our taxonomy (cf. Fig. 2), while each column depicts an LLM under evaluation. Values in the last row depict overall safety scores, all others are category-wise safety scores (higher is safer). Safe scores S (Î¦) â¥ 99 are gray , unsafe scores within 90 â¤ S (Î¦) < 99 are orange , and highly unsafe scores S (Î¦) < 90 are red . Best viewed in color.

<!-- image -->