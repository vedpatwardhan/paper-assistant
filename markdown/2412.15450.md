## Fietje: An open, efficient LLM for Dutch

## Bram Vanroy 1 , 2

bram.vanroy@kuleuven.be

1 KU Leuven, Blijde Inkomststraat 21, 3000 Leuven, Belgium

2 Dutch Language Institute, Rapenburg 61, 2311 GJ Leiden, The Netherlands

## Abstract

This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis of this work is transparency and reproducibility: Fietje is fully open-source, with model weights, datasets, training, and evaluation code all publicly accessible.

The paper discusses the performance of Fietje and many other models on an extensive evaluation suite of benchmarks on reasoning, sentiment analysis, world knowledge, linguistic acceptability and word sense disambiguation. Evaluation results illustrate the rapid progress in the field of LLMs, where recent small models outperform older, larger models that were fine-tuned for Dutch. This trend signals an exciting future for Dutch language processing, suggesting that even compact LLMs are becoming increasingly capable. Furthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to enhance these models even further, broadening their applicability and accessibility. Fietje is only an intermediate step in improving accessibility to language technology for users of the Dutch language.

## 1. Introduction

Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable proficiency across various tasks, including generation tasks as well as zero-shot classification or annotation. However, their performance has predominantly focused on English, leaving other languages underrepresented. For this reason, we introduce Fietje, a continued pretraining model that adapts the predominantly English-focused phi-2 (Javaheripi et al. 2023) Small Language Model (SLM) of 2.7 billion parameters to Dutch alongside instruction and chat variants. By training on 28 billion Dutch tokens sourced from open, filtered web data, Fietje enhances its proficiency in the Dutch language.

Since its initial release in April 2024, many new LLMs have been published. More and more, LLMs are moving away from English-only data and have started incorporating multilingual capabilities, such as the Phi 3.5 (Abdin et al. 2024), Qwen 2.5 (Qwen Team 2024) and Llama 3.1, 3.2 and 3.3 (Grattafiori et al. 2024) models.

In addition to describing the creation of Fietje and the datasets used, it is evaluated on key benchmarks in reasoning, knowledge, sentiment analysis, linguistic acceptability, and word sense disambiguation. For comparison, other models are also evaluated. These models cover different sizes (2-7B parameters) as well as English-centric, multilingual, and Dutch-adapted models. The benchmark results reveal that at the time of publication Fietje surpassed expectations for its size and showed performance that is at times competitive with models twice its size. However, at the same time the benchmarks also illustrate the rapidly changing landscape of LLMs, and how small, multilingual models are now outperforming even Dutch-specific models. At the end, a reflective discussion section posits some points of focus for future models for Dutch, and a limitation section summarizes shortcomings of Fietje and its evaluation.

By emphasizing transparency and openness through the release of data, training and evaluation code, and model weights, this work aims to advance the development and assessment of efficient, high-performing LLMs and SLMs for underrepresented languages like Dutch. Training

code and evaluation be accessed via Github on https://github.com/BramVanroy/fietje-2 and https://github.com/BramVanroy/clin34-benchmarks respectively. Models, including quantized versions, and data are available on the Hugging Face hub https://huggingface.co/collections/ BramVanroy/fietje-2-662cb803ed5cc4f617404146 .

## 2. Related work

Adapting large language models (LLMs) to new languages has garnered significant attention, especially as models trained primarily on English often exhibit performance limitations in other languages. To improve access to language technology to other languages, researches have looked into adapting existing English-focused models to other languages. Strategies for language adaptation generally fall into three categories: tokenizer updates, partial model retraining, and input-level modifications.

## 2.1 Tokenizer updates

Remy et al. (2024) propose a cross-lingual vocabulary transfer strategy called trans-tokenization. This method initializes token embeddings in the target language using a weighted average of semantically similar tokens from the source language. By leveraging a parallel corpus, trans-tokenization achieves efficient vocabulary adaptation, enhancing performance in low-resource scenarios without extensive retraining. Similarly, Csaki et al. (2024) demonstrate that augmenting the tokenizer vocabulary with language-specific tokens can reduce the number of subwords needed to encode words in the target language. However, while these methods improve tokenizer efficiency, the performance gains on downstream tasks remain inconsistent. The Dutch Tweety model proposed in Remy et al. (2024) is part of the benchmark suite in this paper.

## 2.2 Model retraining

Another efficient strategy involves retraining the embedding layer while keeping the transformer layers frozen. de Vries and Nissim (2021) adapted GPT-2 for Dutch and Italian by retraining the lexical embeddings. This approach minimizes computational costs and retains the original model's knowledge. Nevertheless, the method can still result in syntactic and lexical errors when significant linguistic divergence exists between the source and target languages.

Continued pretraining on target-language corpora is a powerful approach to adapt LLMs to new languages. Toraman (2024) applied continual pretraining to English-centric models for Turkish, observing significant gains in language comprehension and downstream performance. However, interestingly, for Turkish they note that vocabulary extension alone offers negligible benefits. For Dutch, Vanroy (2023) attempted to retrain Llama 2 but the real performance improvements in the Dutch LLM space only broke through with the release of GEITje, a continued-pretrained version of Mistral 7B v0.1 (Jiang et al. 2023) for Dutch (Rijgersberg and Lucassen 2023). Later, GEITje was further improved through preference alignment (Vanroy 2024, GEITje 7B Ultra). These models outperformed previous adaptations due to their extensive use of Dutch-specific data.

Continue-pretraining is expensive as it often involves training on large corpora. Techniques like QLoRA (Dettmers et al. 2023) reduce computational requirements by fine-tuning only a limited number of parameters. However, Vanroy (2023) and Toraman (2024) highlight that QLoRA, while efficient, may not achieve the same quality as full retraining, particularly when adapting to languages with complex morphology.

## 2.3 In-context learning and prompt engineering

Rather than retraining any of the parameters of a model, in-context learning offers a way to adapt LLMs to new languages by prompting. Zhang et al. (2024) introduce a framework, which adapts

LLMs to new languages on the fly by providing dictionaries and a small number of parallel examples in the prompt. Their approach enables translation and other tasks without any model parameter updates, achieving significant performance gains for unseen low-resource languages.

Similarly, cross-lingual prompting methods have also been explored. Huang et al. (2023) propose cross-lingual 'thought prompting', where models are guided through language-specific prompts to enhance multilingual reasoning, intermediately translating queries to a different language. This is similar to the work of Qin et al. (2023), who show that translating prompts into a high-resource language (e.g., English) and applying chain-of-thought reasoning can improve performance on multilingual tasks. These approaches leverage the model's stronger performance in languages it was originally trained on, circumventing the need for retraining.

## 2.4 Hybrid methods

Combining multiple adaptation strategies often yields the best results. For instance, Remy et al. (2024) and Toraman (2024) suggest that combining tokenizer updates with continued pretraining can mitigate the weaknesses of either method used in isolation. Similarly, Joshi et al. (2024) advocate for integrating efficient fine-tuning techniques like LoRA with careful prompt engineering to maximize performance in low-resource scenarios while at the same time minimizing the need for large quantities of computational power.

Future research directions may focus on developing more efficient cross-lingual transfer techniques and improving the scalability of hybrid adaptation strategies. Some insights gathered from this paper and related work will be discussed in the Discussion.

## 3. Model creation

Three different flavors of Fietje were created: a base model for text completion, an instruct version for instruction following, and a chat version for an improved assistant experience. All models were trained on hardware provided by the Flemish Supercomputer Center. 1 As for training code, the Alignment Handbook (Tunstall et al. 2024) Github repository already allowed for instruction tuning (supervised funetuning or SFT) and preference tuning. I updated the code to enable continued pretraining and merged our changes to that repository so that other users can also continue-pretrain their models with this library. All the model training is fully reproducible thanks to the open data and open code. The necessary configuration file and training instructions can be found on Github: https://github.com/BramVanroy/fietje-2/tree/main/training .

## 3.1 Base

Architecture Fietje's base model is built on the foundation of Phi 2 (Javaheripi et al. 2023) 2 , a Transformer (Vaswani et al. 2017) decoder model with a context length of 2048 tokens. It is a base model with no explicit chat or instruction tuning, although the model README file suggests three formats that work best for question-and-answering, chat between fictional characters, and coding. Phi 2 was released in the middle of December 2023 as a small language model of 2.78 billion parameters that achieved performance on the level of much larger models. It was trained with an emphasis on the dataset of 250 billion tokens, totalling 1.4 trillion tokens after over-sampling. Microsoft's approach for Phi 2 was built on their early work with Phi 1 (Gunasekar et al. 2023) and Phi 1.5 (Li et al. 2023), where they highlighted the importance of high-quality, curated data rather than overly relying on massive-scale, but potentially poor quality, web data. In that work, aptly titled 'Textbooks are all you need', they showed remarkable performances on benchmarks, not by increasing parameter count or simply scaling compute and data, but by filtering existing code or

web data on its quality, and by generating synthetic data of diverse topics. Unfortunately Phi 2 is an English-centric model. Multilingual Phi models only saw the light of day starting with Phi 3.5, released in August 2024, long after Fietje's release. Therefore, Fietje set out to adapt Phi 2 to Dutch. Phi 2 was selected as the starting point because at the time it was the best performing small language model (less than 3 billion parameters).

Data To adapt Phi 2 to Dutch, it was continue-pretrained on 28 billion high-quality Dutch tokens. For comparison: GEITje 7B, a larger model continue-pretrained for Dutch based on Mistral was trained on 10 billion tokens (Rijgersberg and Lucassen 2023), similar to Boreas 7B, 3 another Mistral 7B (Jiang et al. 2023) finetune. So even though Phi 2 is a much smaller model, it was continuepretrained on much more Dutch specific data to ensure a high-quality, small model. The training dataset consists of Dutch Wikipedia (dump of November 2023 4 ) and was further expanded with random samples from the CulturaX dataset (Nguyen et al. 2024). CulturaX is a high-quality, multilingual dataset covering 167 languages. Its creation is thorough, including URL-based filtering as well as metric-based cleaning (e.g., perplexity score, characters per document, number of lines) and deduplication. Before training, Dutch CulturaX was filtered even further (Wikipedia was not filtered in this way, as discussed later):

- · removed documents that contain the text 'rechten voorbehouden' or 'rights reserved' to avoid including explicitly copyrighted materials;
- · remove documents whose URL contained 'wikipedia.org' (because a very clean version of Wikipedia was included separately);
- · removed documents that contain a 'bad word' (see Appendix A; this appendix contains offending words!);
- · removed documents that contain any non-Latin characters. This is a very strict filter; the assumption here is that we only 'trust' a curated knowledge-database like Wikipedia to contain non-Latin script (e.g., to describe the original name of a person or place). General web data with non-Latin texts is filtered out, as a harsh quality heuristic.

While CulturaX alone is therefore filtered relatively strictly, a second step of filtering also includes the Wikipedia data. The following heuristics were calculated on the SONAR-500 corpus to serve as a baseline of 'high quality data' and manually checked for its applicability (excluding more noisy components WRPEA, WRPED, WRUEA, WRUED, WRUEB). The following measures were taken:

- · removed documents where ratio of punctuation marks vs. non-whitespace characters is higher than 0.2;
- · removed documents where ratio of uppercase vs. non-whitespace characters is higher than 0.22;
- · removed documents where ratio of digits vs. non-whitespace characters is higher than 0.16;
- · removed documents where the average token length is less than 2 or greater than 20.

While these filters are a useful starting point, more thorough, Dutch-specific quality filters or classifiers will be needed to ensure high-volume, high-quality datasets to efficiently train Dutch LLMs.

While only a subset of 28 billion tokens was used for Fietje, larger subsets (up to 55B tokens) of this filtered CulturaX and Wikipedia mix are also made available. The full dataset is available at https://huggingface.co/datasets/BramVanroy/wikipedia\_culturax\_dutch .

Training Fietje was trained for around two weeks on four nodes of four A100 80 GB GPUs each (16 total). Note, however, that due to a CUDA configuration bug, the actual processing time should have been much shorter. Training is reproducible with the configuration file provided in Appendix B.1, also available on the aforementioned Github URL. The model weights can be found on the Hugging Face Hub. 5

## 3.2 Instruct

Fietje itself is only capable of text completion, so to adapt it for instruction following, it was further trained with supervised finetuning on semi-structured data, namely Dutch conversations. The result of this process is Fietje Instruct. 6

Data Although large datasets for instruction following are lacking in native Dutch, a number of synthetic datasets exist. UltraChat 200K Dutch 7 and No Robots Dutch 8 are two datasets that were introduced by Vanroy (2024). They were both generated with GPT-4. UltraChat 200K Dutch contains conversations that have multiple user-assistant turns. Furthermore, it was created with user accessibility in mind: the 'user' messages in the dataset are written by GPT-4 taking on a 'persona' such as a language learner, a critic, or a curious child. The intention is that a model finetuned on this data is better equipped to handle such diversity of users.

Belebele (Bandarkar et al. 2024) is a dataset intended for multiple-choice reading comprehension. For this instruction-tuning step, it was converted into the appropriate conversational format. 9 While small, the dataset was created by humans, ensuring its high quality.

In total, these datasets amount to 201,579 conversations.

Training The instruction tuned version of Fietje was trained for around a day on four nodes of four A100 80 GB GPUs each (16 total). The training configuration can be found on Github or in Appendix B.2. Note that the configuration ensures that the ChatML chat template is applied.

## 3.3 Chat

As a last step in the creation process, Fietje Instruct was also 'aligned' with preference data, specifically using the Direct Preference Optimisation algorithm (Rafailov et al. 2024). In this process the model is provided with a prompt and a preferred reply and an unwanted reply, and it will learn that the content, style, or way of replying of the preferred answer is something to mimic whereas the unwanted reply should be avoided.

Data Similar to the datasets for Dutch instruction tuning, no large, native Dutch preference datasets exist. Synthetic datasets, are available though. Fietje Chat 10 was trained on 18,653 preference pairs taken from UltraFeedback Dutch Cleaned as well as Orca DPO Pairs Dutch Cleaned, both described in Vanroy (2024). The dpo hq subset was taken from UltraFeedback 11 , which contains Dutch prompts that were answered by GEITje 7B Ultra and GPT-4. GPT-4 was then asked to rate the responses on the quality of the Dutch language, their helpfulness and conciseness - having LLMs rate their own responses is an intriguing but common practice. The best rated response, GEITje or GPT-4 was then used as the 'preferred' response and the other as the unwanted one. Similarly, for Orca DPO Pairs 12 , the dpo all subset was used for training. This subset was not rated, so GPT-4 was always selected as the preferred response and GEITje as the worst response.

Training Since the DPO training set was quite small, only one A100 80GB was used. One training run took around 9 hours. However, similar to the experience of Vanroy (2024), it was found that DPO's beta parameter was difficult to tune while avoiding hallucinations or catastrophic forgetting. This parameter controls how close the model should stick to its starting point, or how far it should move away. The final beta value of 0 . 2 was selected after hyperparameter tuning. The full configuration is given in Appendix B.3 and on Github.

## 4. Evaluation

In this section we will evaluate Fietje and its instruction and chat variants on a number of benchmarks, and compare its results with other language models, some of which are also specifically tailored to Dutch, others which are multilingual. Plenty of models are given to highlight a few insights when it comes to model release date (multilingual performance has improved significantly since Fietje's release), model sizes, and the impact of adaptation to Dutch.

Crucially, it is important to emphasize that Dutch, and many non-English languages, has been struggling with LLM evaluation. As discussed before in (Vanroy 2023, Vanroy 2024), most existing benchmarks are translated, are 'simply' classification problems that do not measure the fluency of the model, and/or are not localized to the intricacies of Dutch language culture. This is discussed in more detail in Section 4.3.

## 4.1 Framework

While evaluation frameworks for generative large language models exist, such as ScandEval (Nielsen 2023) and the LM Evaluation Harness (Gao et al. 2024), they either did not provide the benchmarks (for Dutch) that I wanted to emphasize, did not allow ease-of-customizability with configuration files, or did not include functionality such as speed and fertility tests that I needed. Notably, while ScandEval specifically is a useful resource for comparing models, it only evaluates on a limited number of tasks, and - crucially - some tasks are only evaluated on parts of the test set rather than the whole test set to save on computational costs, which is not always clear as a viewer of the 'leaderboard'. Therefore, an extensible evaluation framework is made available accompanying this paper to reproduce all results presented here. The benchmark code (including fertility and throughput calculation) and accompanying benchmark configuration files are available at https: //github.com/BramVanroy/clin34-benchmarks for full reproducibility.

To enable efficient guided generation in the benchmarks, Outlines was used as a backbone of the benchmarking suite (Willard and Louf 2023). This library enables you to constrain a model's output by regular expressions, JSON schema, or simply a list of options. In zero-shot benchmarks like the ones discussed here, that means that the model will always predict one of the allowed labels and is not able to hallucinate labels or responses. For instance, for a sentiment analysis task the prompt may be in the line of 'Would you say that the following book review is 'positive' or 'negative'?' and in the benchmarking code we can explicitly state that the model is only allowed to return 'positive' or 'negative', leading to clean benchmarking results that do not require any post-processing.

In the benchmarks, sampling was enabled (no top p or top k selection, and temperature 1). That means that the model's predictions are limited thanks to Outlines to the list of valid labels (e.g. 'positive' and 'negative') but that due to sampling, the predicted outcome are still randomly sampled, but weighted according to their initial probability. Since each prediction is run multiple times, the confidence interval will therefore be an important metric to consider. If the model is 99% confident that 'positive' is the right answer, then the confidence interval will be small because even with random sampling, 'positive' will be the most picked answer. However, if the model is only 50% confident, then the confidence interval would likely be much larger since the random sampling will lead to a 50/50 choice on the labels. Therefore, large confidence intervals will be indicative of a

model's poor confidence, as the name implies. Each benchmark was run five times to compute the confidence intervals.

Benchmarks were run in bfloat16 precision with flash attention 2 enabled (Dao 2024). All benchmarks were run on four RTX 3090 with 24 GB of VRAM each.

## 4.2 Model overview

To compare the Fietje models, a number of different large language models were selected for comparison. Many of these were released after Fietje - so the expectation is that newer models will perform better - but including them highlights the rapid changes in the (multilingual) LLM world. In Table 1, notable model characteristics are given. The columns are described in the table's caption. In the Wikipedia columns, fertility (the average ratio of how many subword tokens are needed to encode a word) is calculated on full Dutch Wikipedia (cleaned dump of November 2023) 13 , whereas the tokens-per-second (wiki tps ) and processing time in seconds (wiki s ) were calculated on the first 10,000 documents of Dutch Wikipedia. The data and training transparency columns are to indicate how reproducible a model is: is the data specifically described and publicly available, and is the training code or configuration provided?

Phi 2 and derivatives Phi 2 (Javaheripi et al. 2023) serves as the base model for Fietje. It was discussed before in Section 3. Phi 2 and its derivatives are the smallest model in the list in terms of parameter count. Its tokenizer is a bit worse than other models in terms of fertility: on average 2.05 subword tokens are needed to encode a single Dutch word, highlighting that it was intended for English. Still, given its small size it is the fastest model in the list: Phi 2 related models can process most tokens per second and are also the fastest in processing time of Wikipedia. While all Fietje models are fully reproducible (public code, public data), that is not the case for Phi 2, which is not specific about the data or training regimen used.

Mistral v0.1 Mistral v0.1 (Jiang et al. 2023) is a 7.2-billion-parameter language model known for its performance in monolingual English benchmarks at the time. Mistral v0.1 was not trained on multilingual data and lacks optimizations for the Dutch language according to their model card, which only mentions English. Consequently, its fertility on Dutch text is relatively high, averaging 1.97 subword tokens per word. Combined with its large parameter count, this leads to the slowest throughput in terms of Wikipedia tokens per second ( tps ) among the models compared. Despite its technical advancements at the time, Mistral v0.1 suffers from a lack of transparency regarding its training data and procedures. While the model architecture and code are available under an open-source license, the exact data sources and training configurations remain undisclosed, limiting the reproducibility of its results.

Prior efforts were taken to make Mistral v0.1 more fluent in Dutch by continue-pretraining the model without changing the tokenizer. The first successful attempt in this regard was the creation of GEITje (Rijgersberg and Lucassen 2023), a continued-pretrained version of Mistral on 10 billion Dutch tokens which have been clearly described though not all publicly available. The training code is publicly available. GEITje 7B was later extended by (Vanroy 2024) into 'GEITje 7B Ultra' by finetuning the model on new, open instruction datasets as well as aligning the model with preference dating using Direct Preference Optimisation (Rafailov et al. 2024). More recently, another Mistral v0.1 continued-pretrained model was launched called Boreas 7B 14 and its instruct version called Boreas 7B Chat (which is an instruction-tuned model, not a preference-tuned model). It was trained on a mix of 10 billion English and Dutch tokens with an extensive finetuning phase for instructiontuning with another 4.7B tokens, much more than the instruction tuning for GEITje. The datasets are a combination of public and private datasets that include lots of Dutch literature, news and

educational materials. Training code is not available but training hyperparameters are discussed in the model card.

Tweety Although the Dutch 7B Tweety model (Remy et al. 2024) is also a derivative of Mistral v0.1, it is special. As described in Section 2, Tweety has an updated tokenizer and vocabulary to tailor it better to the Dutch language, as is clear from its outstanding low fertility. This is clear in the architecture as well as in its efficiency: while it is a bit larger than other Mistral derivatives (7.4B vs 7.2B parameters), leading to a slower throughput (wiki tps ), it is still much faster in processing Wikipedia (wiki s ). In other words: because its tokenizer is optimized for Dutch, it needs fewer subword tokens to encode the same information, and therefore can process the same text much more efficiently. After updating the tokenizer, the model was 'finetuned' on 400 million Dutch tokens from the mC4 corpus (Raffel et al. 2020) to adapt to the new embeddings, although the exact subset is not known. The code for updating the tokenizer and model is available 15 but the finetuning code is not.

Phi 3.5 The Phi 3 and Phi 3.5 models (Abdin et al. 2024) represent a significant evolution in the Phi family, with Phi 3.5 incorporating multilingual data during mid-training. However, the exact composition of languages in the training corpus is not disclosed - in fact, no languages are explicitly listed, resulting in a lack of transparency regarding the data sources and training procedures. Despite this, these models exhibit improved tokenizer performance compared to Phi 2, with a fertility of 1.89 subword tokens per Dutch word, indicating a better adaptation to the Dutch language. This enhancement contributes to more efficient processing of Dutch text. In this paper we discuss Phi 3.5 Mini Instruct, a 3.3B version of Phi 3.5.

Qwen 2.5 Qwen 2.5 (Qwen Team 2024) is a series of multilingual language models trained on over 29 languages. In this paper, for comparison with Fietje, the 3.1-billion-parameter instruction variant is selected. Although Dutch is not explicitly mentioned among the language list (in fact the 29 languages are not listed in full), the model demonstrates a commendable tokenizer fertility of 1.82 subword tokens per Dutch word, surpassing both Phi 2 and Phi 3.5. This efficiency, coupled with its relatively small size, enables Qwen 2.5 to process Dutch Wikipedia text with fast speed. However, the model's transparency is limited, as details regarding its training data and code are not publicly available, hindering reproducibility and understanding of the model creation.

Llama 3.2 The Llama 3.2 3B Instruct model is part of Meta's Llama 3 series (Grattafiori et al. 2024), which supports multilingual training across eight languages. Although Dutch is not explicitly listed, the tokenizer exhibits strong performance on Dutch text, second only to Tweety's Dutchoptimized tokenizer. With a fertility notably low for a general-purpose model, Llama 3.2 processes Dutch Wikipedia almost as efficiently as Phi 2, despite being 14% larger in parameter count. This balance of size and efficiency results in high throughput for Dutch text processing. The Llama 3 paper stands out for its transparency, offering extensive insights into model creation, data curation, and technical training details. While the exact data mix and training code are not provided, the paper describes a rich blend of code, math, multilingual, and filtered web data, including references to specific public datasets.

s

wiki

tps

wiki

y

fertilit

wiki

0.03

0.03

±

0.22

±

440.41

0.16

±

440.41

0.66

0.02

±

440.40

0.66

±

0.73

±

999.42

4.72

±

9501.41

2.05

±

451.97

0.64

±

9501.41

2.05

0.06

0.28

±

±

434.44

0.36

±

9501.70

2.05

584.14

16.12

±

4035.27

1.97

0.68

±

7884.63

1.74

1001.27

0.16

±

0.03

0.41

±

±

996.66

1.14

±

9631.95

2.05

459.94

0.67

±

6633.85

1.89

0.53

±

4027.81

1.97

1002.82

0.39

3.78

±

±

728.56

1.64

±

4046.46

1.97

2.12

±

8094.26

1.82

1000.23

0.17

±

999.65

15.28

±

4021.61

1.97

0.69

±

3979.88

1.41

±

4032.05

1.97

4034.36

1.97

data/training

h?

Dutc

for

ecifically

ailable?

v

a

publicly

de

co

training

and

t.

efficien

more

=

er

w

Lo

edia.

Wikip

h

in

3090

TX

R

isolated

an

on

calculated

cumen

date size t yp e Dutc 4/24 2.8B base y es 4/24 2.8B c hat y es 4/24 2.8B instruct y es 1/24 7.2B c hat y es 9/24 3.2B instruct no 12/23 2.8B base no 8/24 3.8B instruct 9/23 7.2B instruct no 9/23 7.2B base no 9/24 3.1B instruct 12/23 7.2B base y es 5/24 7.4B base y es 4/24 7.2B base y es 4/24 7.2B instruct y es of b enc hmark ed mo dels. Dutc the data and training pro c edur w man y tok ens are needed on a v er-second throughput on first of first 10,000 Wikip edia do A tten tion 2 enabled. Batc h size their CI are based on the results

wiki

second.

er

p

ess

c

pro

l

e

d

mo

the

orted

rep

The

most.

at

8192

or

used,

fietje-2b-instruct

GEITje-7B-ultra

Llama-3.2-3B-Instruct

undersp

Phi-3.5-mini-instruct

Mistral-7B-Instruct-v0.1

undersp

en2.5-3B-Instruct

Qw

h-v24a

y-7b-dutc

eet

w

t

erview

Ov

1:

able

T

are

:

transparency

ho

:

y

fertilit

wiki

ens-p

ok

T

:

tps

wiki

time

sing

ces

Pro

:

s

Flash

with

bfloat16

and

metrics

mean

name

fietje-2b

hat

fietje-2b-c

phi-2

Mistral-7B-v0.1

GEITje-7B

Boreas-7B

hat

Boreas-7B-c

## 4.3 Benchmark overview

The following benchmarks are considered to cover different aspects of LLM capabilities: ARC (reasoning), DBRD (sentiment analysis), Dutch CoLA (grammar/linguistic acceptability), Global MMLU (language understanding and world knowledge), XLWIC-NL (word sense disambiguation). Argumentation for this selection of benchmarks, and why others were not included, is given in Section 6.2.

ARC The AI2 ARC (AI2 Reasoning Challenge) dataset (Clark et al. 2018) consists of 7,787 natural, grade-school science questions in advanced reasoning beyond simple fact retrieval. The dataset is divided into two parts: an Easy Set (5,197 questions) and a Challenge Set (2,590 questions). This English challenge dataset was translated with GPT-3.5-turbo to a multitude of languages, including Dutch, by Lai et al. (2023). The dataset is presented to a model as multiple choice questions.

DBRD The Dutch Book Reviews Dataset (van der Burgh and Verberne 2019) is a sentiment analysis dataset based on actual book reviews taken from the website hebban.nl. Each review was accompanied by a score out of five. The reviews with a score of 4 and 5 were labeled as 'positive' while those marked 1 or 2 were labeled as 'negative'. The dataset contains 2,224 test samples.

Dutch CoLA The Dutch CoLA dataset 17 (Bylinina et al. 2024) is a linguistic acceptability corpus for Dutch, following the structure of the English CoLA dataset (Warstadt et al. 2019). It comprises sentences labeled as grammatically correct or incorrect, derived from expert-annotated examples found in published Dutch grammar literature. The dataset contains 2,400 test sentences.

Global MMLU The Global MMLU (Massive Multitask Language Understanding) benchmark (Singh et al. 2024) is an extension and improvement of the original English MMLU (Hendrycks et al. 2021), designed to evaluate LLMs across multiple languages, including Dutch. This benchmark measures a model's ability to perform on a wide range of academic and professional knowledge tasks spanning STEM, humanities, social sciences, and more, with questions ranging in difficulty from elementary to expert levels. Questions are formulated as multiple choice questions.

While the original MMLU focuses solely on English, the Global MMLU provides translated and culturally verified versions of the dataset. The Dutch portion was generated by first translating the English MMLU questions using machine translation and subsequently improving the quality through human post-editing. Note that this improved MMLU version is different from earlier machinetranslated versions of MMLU by Lai et al. (2023).

XLWIC-NL The multilingual Word-in-Context dataset (Raganato et al. 2020) is intended for word sense disambiguation of nouns and verbs. The Dutch portion is derived from the Dutch WordNet, which provides curated sense inventories and example usages, ensuring reliability in distinguishing different word senses. A model is presented with a target word and two sentences where the word (or its conjunction) is used. The model should then predict whether the meaning of the word is identical or different in the two sentences. The test set includes 1,004 test instances.

## 4.4 Results

In this section, the performance of the models on the selected benchmarks is presented and discussed from different points of view. While the focus of this paper lies on Fietje and its derivatives, the benchmark results also give food for thought about other models and the tasks themselves.

rank

median

WIC

XL

rank

WIC

XL

C

AR

on

times

e

fiv

hmark

enc

b

h

eac

running

y

b

1.0

8

0.31

2.0

12

0.38

±

3.5

14

0.34

±

37.39

2

0.22

3.5

3

0.76

±

36.05

1

0.45

5.0

4

0.53

±

33.78

3

0.66

6.0

1

0.79

±

42.72

4

0.93

40.03

5

1.06

7.0

2

1.07

8.0

5

0.94

±

±

44.45

8

0.56

43.23

7

1.25

9.5

10

0.98

±

9.5

6

1.00

±

39.24

6

0.78

11.0

9

0.88

±

37.24

12

0.85

12.0

7

0.75

±

38.61

9

0.82

13.0

11

0.97

±

37.27

11

0.85

14.0

13

0.70

±

37.64

10

1.30

±

36.55

14

0.52

34.28

13

0.89

als.

terv

in

confidence

erlapping

v

o

considering

Global MMLU MMLU rank 48.34 ± 0.10 2 50.33 ± 0.14 1 44.93 ± 0.15 3 35.59 ± 0.22 4 32.30 ± 0.38 5 24.39 ± 0.18 12 27.36 ± 0.32 7 26.36 ± 0.25 9 27.02 ± 0.63 8 24.93 ± 0.27 11 27.51 ± 0.15 6 25.12 ± 0.37 10 20.82 ± 0.28 14 24.09 ± 0.43 13 s h o wing w eigh are also giv en, the final median

Phi-3.5-mini-instruct

en2.5-3B-Instruct

Qw

Llama-3.2-3B-Instruct

Mistral-7B-Instruct-v0.1

h-v24a

y-7b-dutc

eet

w

t

results,

hmark

Benc

2:

able

T

ranks

dels'

Mo

del).

mo

h

eac

illustrates

column

last

The

Global

rank

hat

Boreas-7B-c

GEITje-7B-ultra

hat

fietje-2b-c

Boreas-7B

fietje-2b-instruct

Mistral-7B-v0.1

GEITje-7B

phi-2

fietje-2b

Table 2 can be analysed in a number of ways:

Impact of model size and release date While a realistic expectation would be that larger models perform better overall, this is not the case, as visualized in Figure 1a. Even limiting the model selection to only those models that were not specifically adapted to Dutch (Fig. 1b), no trends are visible: larger models like Mistral 7B are outperformed by much smaller ones like Qwen 2.5 3B. Detailed visualizations for the relationship between model size and individual task performance are given in AppendixD.1.

Figure 2: Model release date vs. median performance

<!-- image -->

<!-- image -->

(b) Without modified models

Figure 1: Model size vs. median performance

However, when considering the release date of the models (Fig. 2a), it becomes clear that more recent models tend to have an advantage over older models (considering their respective model sizes). This becomes very clear when focusing on the un-adapted models in Figure 2b, where newer models like Phi 3.5 and Qwen 2.5 have an edge over Phi 2, and even over the much larger but older Mistral v0.1 models. Appendix D.2 plots the model release date on their performance of specific tasks for the interested reader.

<!-- image -->

(a) All models

<!-- image -->

(b) Without modified models

Note on confidence intervals and rank First and foremost it must be acknowledged that many models exhibit tight, sometimes even overlapping, confidence intervals, suggesting that performance differences in some tasks are marginal between certain models. While ranking models on their performance provides a quick way of putting them on a leaderboard, the ranks should be interpreted with caution. For instance, in the DBRD (sentiment analysis) benchmark, Phi 3.5 and Qwen 2.5 have confidence intervals that slightly overlap, making their ranking potentially interchangeable.

Fietje Fietje's results provide several insights into its capabilities and limitations. First and perhaps most surprising, the base Fietje model sometimes underperforms compared to Phi 2, the model it was derived from, although the confidence intervals suggest these results are close enough to be considered a toss-up. However, Fietje also notably outperforms Phi 2 on reasoning tasks like ARC (reasoning; Fig. 3a) and MMLU (knowledge and understanding; Fig. 3d), indicating successful adaptation for knowledge-based tasks.

The instruct and chat versions of Fietje show substantial improvements over the base version. Fietje 2B Chat, in particular, performs remarkably well for its size, outperforming larger 7B models like GEITje and Tweety on multiple benchmarks, including ARC and MMLU. Specifically, Fietje Chat surpasses GEITje Ultra and Tweety in two out of five tasks. This demonstrates the efficacy of instruction tuning and chat-specific adaptations in enhancing model performance. Especially at the time of its release, when models like Boreas, Phi 3.5, Llama 3.2 and Qwen 2.5 did not exist yet, Fietje showed to clearly perform as the best model on Dutch in its weight category.

Performance of modern, small, multilingual models The benchmark results highlight the exceptional performance of small, multilingual models like Qwen 2.5 and Phi 3.5. This improved performance on Dutch benchmarks illustrates a welcome trend: size is not the sole determinant of model performance but multilingual pretraining efforts are key. These models, both under 4 billion parameters, outperform several 7B models on benchmarks like MMLU and ARC. Indeed, looking at the corresponding Figures 3d and 3a, very similar tendencies between models can be observed, and the top three models are visibly a step ahead of the others. While Phi 3.5, Qwen 2.5 and Llama 3.2 all perform well, a clear gap exists between the first two and Llama 3.2, which performs worse than the others. This gap may be attributed to the training data: while it is unclear (but likely) whether Phi 3.5 and Qwen 2.5 trained on Dutch, the Llama 3.2 model card explicitly mentions that it is only trained on English, German, French, Italian, Portuguese, Hindi, Spanish and Thai - no Dutch. Yet, intriguingly, its tokenizer has a lower fertility on Dutch data than the others.

Model performance consistency A notable observation in the benchmark results is the consistent performance of the top 5 models across multiple tasks. Qwen 2.5, Phi 3.5, Boreas Chat, Llama 3.2, and Mistral 7B Instruct frequently occupy the leading positions in tasks such as Global MMLU, DBRD, and ARC. This consistency suggests that while these models differ in size and architecture, their training data and multilingual capabilities give them an advantage across diverse benchmarks. The odd-one-out in this respect is Mistral 7B Instruct, which was not explicitly trained on Dutch.

Performance on XLWIC However, even despite this consistency, one task catches the eye. The XLWIC benchmark, which tests semantic disambiguation, stands out as an anomaly compared to the other tasks (visualized in Figure 3e). Several models that perform well across tasks like ARC or MMLU, such as Boreas Chat and Qwen 2.5, perform poorly on XLWIC. For instance, Boreas Chat ranks worst on XLWIC despite being in the top 4 for other tasks. While it is tempting to argue that XLWIC is simply a task that requires intricate knowledge of the Dutch language, explaining why Dutch-adapted models such as GEITje 7B Ultra and Tweety perform so well, this does not hold: Mistral 7B Instruct ranks 4th on this task, even though it was not trained on Dutch.

Performance gaps between worst and best model Performance gaps between models are particularly pronounced in the ARC (reasoning; Fig. 3a) and DBRD (sentiment analysis; Fig. 3b) benchmarks. For ARC, the top-performing model, Qwen 2.5, achieves a score of 66.97%, while the worst-performing model, Phi 2, scores only 18.07%, reflecting a significant disparity in reasoning skills. A similar trend is visible in DBRD, where Boreas Chat achieves the highest score (94.38%), incredibly close to the state-of-the-art for the benchmark of 95.14% F1 (Delobelle et al. 2020), which required finetuning a Dutch encoder model on the task. On the other end of the spectrum, Phi 2 only reaches 51.45%. In the case of Boreas Chat, this may not be a surprise since it was trained on an undisclosed set of Dutch literature. In fact, out of all Mistral derivatives, Boreas is the best-

Figure 3: Results per benchmark

<!-- image -->

ranking model, which is likely due to its varied Dutch-English dataset. A curious exception is its poor performance in the disambiguation task WIC, where it scores worst of all models (Fig. 3e).

Mistral 7B Instruct's surprising performance The Mistral 7B Instruct model demonstrates strong overall performance, with the notable exception of the Dutch CoLA benchmark (Fig. 3c), where it ranks last among all models. Despite being older and not being trained explicitly on Dutch according to the model information, it outperforms several Dutch-specific models. This indicates that Mistral's general training data may still provide a reasonable understanding of Dutch, although grammatical nuances remain a challenge. Of course it is also possible that the brief model description neglects to mention multilinguality when in fact the training data was multilingual, however the higher (worse) tokenizer fertility does not point in that direction.

## 5. Discussion

In this work, I have presented Fietje, a continue-pretrained version of Phi 2. The creation of Fietje is marked by an emphasis on reproducibility and transparency. Fietje's training and the evaluations presented here are completely reproducible. Similarly, the data is publicly available.

Fietje proved to be a competitive model for its size when it was launched, exhibiting results comparable to or exceeding GEITje 7B Ultra on benchmarks such as MMLU, Dutch CoLA, and ARC. Yet it was surpassed relatively quickly in the months after its release. The benchmark results presented in this paper illustrate that developments in LLM move quickly and that recent endeavors by large actors (like Meta AI and Microsoft) are thankfully incorporating more multilingual material, even in their smaller models. At the same time, the results also show the promise of language adaptation through language-specific changes of a tokenizer (Remy et al. 2024) and continue-pretraining on high-quality data (Boreas). A combination of the two would likely lead to a model of very high quality.

Not only is high quality of importance, but continue-pretraining on both English and Dutch as done in Boreas may continue to achieve better alignment between the initial model and the new target language. In addition, it is clear that Boreas' data mix has a significant impact on its performance: it clearly outperforms other Mistral v0.1 derivatives in most benchmarks thanks to its undisclosed dataset and/or mixing in English.

Another aspect that deserves more attention is the role of post-training, the stage after pretraining (instruction tuning and potentially alignment). Recent models such as Llama 3 (Grattafiori et al. 2024) and Boreas highlight the significance of extensive post-training procedures, which can significantly improve performance across benchmarks. Yet, for Dutch, public datasets for any form of post-training are sorely scarce.

Focusing on the benchmarks themselves, it is worth emphasizing that machine-translated benchmarks like ARC and Global MMLU, may inadvertently favor some models due to 'translationese effects'. These artifacts of translation can introduce stylistic biases that benefit models trained on similar data. For this reason, other machine-translated benchmarks were not included in this paper (see Sec. 6.2. We should therefore aim to create new, human-created (or at least human-corrected) benchmarks.

In addition, quantitative benchmarks offer valuable insights, but it is important to distinguish between language understanding and language production in large language models. As discussed by Vanroy (2023) and Vanroy (2024), models trained predominantly on English data may perform well on classification tasks in other languages but may struggle with fluent language generation. For example, non-English benchmarks often require models to produce simple labels such as 'positive' or 'negative', or to select multiple-choice answers ('A', 'B', etc.). Such tasks differs fundamentally from generating fluent, grammatically correct Dutch text. While such models may serve as effective zeroshot classifiers, a robust conversational assistant must excel in both comprehension and language production. Unfortunately, current benchmarks lack comprehensive metrics for evaluating fluency, highlighting a gap that future research must address. It is therefore always recommended to a select an LLM based on a user's needs and not merely by the reported numbers on benchmarks.

In conclusion, this paper has introduced Fietje and its Instruct and Chat derivatives, as well as all materials needed to fully and openly reproduce their creation and evaluation. While it performs well for its size, especially at the time of publication, it is clear that the field has not stood still in the months following its release. This paper has also critically examined a number of others LLMs, both specifically tuned for Dutch or not, and found that model size has less of an impact than a model's release date, since smaller, more recent models are outperforming models twice their size. This tendency indicates a hopeful way forward where small models are also multilingually trained, making language technology more accessible for Dutch language users.

## 6. Limitations

While this paper is mostly a system description paper, it also comes with its set of limitations, both in the way the model was created and how it is evaluated.

## 6.1 Model

While Phi 2 was the best model of its weight at the time of training, this quickly changed. Working on language adaptation of LLMs, one always 'lags behind': once you are done with the language adaptation of the model (collecting and filtering data, setting up training, acquiring compute, training), a new model has already been released that is potentially better suited to start from. Furthermore, Fietje was only trained on Dutch CulturaX and Wikipedia, but current approaches tend to incorporate a lot more math and programming code, e.g. Llama 3 was trained on 25% mathematical and reasoning data and 17% code (Grattafiori et al. 2024). In addition, in the 'post-training' phase (supervised finetuning), it would appear that models such as Boreas Chat also hint towards the importance of high-quality, high-volume instruction tuning. While Fietje Instruct was trained on large, general knowledge instruction sets, this can definitely be improved. Since more such datasets are not publicly available for Dutch, there are many more available for English. So continue-pretraining on both Dutch and English datasets in both the pretraining and post-training phase, like Boreas, is an appealing avenue. Massive pretrain corpora for Dutch of high quality are also a topic of need and interest, which has attracted wider attention, as is evident from the release of the Dutch portion of FineWeb 2 (Penedo et al. 2024).

## 6.2 Evaluation

Upon Fietje's release in early 2024, it was relatively straightforward to evaluate and interpret the results. With the GEITje models as one of the few, if not the only, LLMs that were fluent in Dutch, not too large ( < 10B parameters), and performant in benchmarks, they were the key models to compare with. However, since then many model builders have put more emphasis on multilingualism.

Because of the heterogeneous nature of the models in terms of pretraining data, training mechanism, language focus, and small size differences, it is complex to derive convincing tendencies in the benchmark results. There are so many unknowns about most of these models that it is not evident to pinpoint differences and similarities that may explain performance. Even more so, the benchmark results on the words-in-context benchmark (WIC) illustrate how seemingly top-of-the-line models that perform consistently in the top-3 in other benchmarks, may still falter in specific benchmarks of a different nature. To this end, it may be worthwhile to expand the evaluation scope even further to benchmarks that were not included in this paper, such as the machine-translated HellaSwag (Zellers et al. 2019) benchmark (given a cut-off sentence, pick the best sentence continuation), a part-ofspeech benchmark like CoNLL (Tjong Kim Sang 2002), or a question-answering benchmark like the machine-translated SQuAD (Rajpurkar et al. 2018). However, I did not include these because of the computational cost of running benchmarks for all models (and each benchmark five times to compute confidence intervals), but also because I wanted to limit the number of machine-translated benchmarks in favor of benchmarks that included 'natural Dutch' such as DBRD, Dutch CoLA and XLWIC-NL. CoNLL has the additional issue that the expected output response must be in valid JSON. While tools like Outlines can 'force' JSON output, such a requirement still imposes negative biases to models that were not trained on JSON as much - which leads to the question whether the benchmark is actually measuring question-answering skills or JSON-production prowess. In conclusion, evaluating language models is hard, and for Dutch specifically we are in need of higherquality, Dutch-native benchmarks. Not only because of the Dutch fluency, but also to better take into account the culture of the people that use Dutch as their main language.

An additional limitation of this paper is that the benchmarks are only tested in a zero-shot setting. Few-shot benchmarks used to be needed to improve the chances that the model would

actually return the expected format or label, but with constrained decoding that is no longer as important - though it is of course more than likely that benchmark results would improve with fewshot examples. Secondly, a potential shortcoming is that only one prompt was used for each task. It is possible that one task formulation favors one model better than the other, but it is not feasible to do an extensive search for the most optimal prompt for each model for each task. Therefore the current prompt templates (App. C) were selected based on a consistent and natural formulation across benchmarks.

## Acknowledgments

Fietje and derivatives were trained on the VSC Tier-1 cluster of the Flemish SuperComputer Center (Vlaams Supercomputer Centrum; https://www.vscentrum.be/ ) on grant number 2023 071.

## References

Abdin, Marah, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S'ebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio C'esar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou (2024), Phi-3 technical report: A highly capable language model locally on your phone. https://arxiv.org/abs/2404.14219.

Bandarkar, Lucas, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa (2024), The belebele benchmark: a parallel reading comprehension dataset in 122 language variants, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Association for Computational Linguistics, Bangkok, Thailand and virtual meeting, pp. 749-775. https://aclanthology.org/2024.acl-long.44.

Bylinina, Lisa, Silvana Abdi, Hylke Brouwer, Martine Elzinga, Shenza Gunput, Sem Huisman, Collin Krooneman, David Poot, Jelmer Top, and Cain Weideman (2024), Dutch-CoLA (Revision 5a4196c). https://huggingface.co/datasets/GroNLP/dutch-cola.

Clark, Peter, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord (2018), Think you have solved question answering? try arc, the ai2 reasoning challenge. https://arxiv.org/abs/1803.05457.

Csaki, Zoltan, Bo Li, Jonathan Lingjie Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, and Urmish Thakker (2024), SambaLingo: Teaching large language models new languages, in Saleva, Jonne and Abraham Owodunni, editors, Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024) , Association for Computational Linguistics, Miami, Florida, USA, pp. 1-21. https://aclanthology.org/2024.mrl1.1.

- Dao, Tri (2024), Flashattention-2: Faster attention with better parallelism and work partitioning, The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=mZn2Xyh9Ec.
- de Vries, Wietse and Malvina Nissim (2021), As good as new. How to successfully recycle English GPT-2 to make models for other languages, in Zong, Chengqing, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , Association for Computational Linguistics, Online, pp. 836-846. https://aclanthology.org/2021.findings-acl.74.

Delobelle, Pieter, Thomas Winters, and Bettina Berendt (2020), RobBERT: a Dutch RoBERTabased Language Model, in Cohn, Trevor, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020 , Association for Computational Linguistics, Online, pp. 3255-3265. https://aclanthology.org/2020.findings-emnlp.292.

Dettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer (2023), QLoRA: Efficient finetuning of quantized LLMs, Thirty-seventh Conference on Neural Information Processing Systems . https://openreview.net/forum?id=OUIFPHEgJU.

Gao, Leo, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou (2024), A framework for few-shot language model evaluation. https://zenodo.org/records/12608602.

Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm'an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth

Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V'ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish

Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma (2024), The llama 3 herd of models. https://arxiv.org/abs/2407.21783.

Gunasekar, Suriya, Yi Zhang, Jyoti Aneja, Caio C'esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S'ebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li (2023), Textbooks are all you need. https://arxiv.org/abs/2306.11644.

Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (2021), Measuring massive multitask language understanding, International Conference on Learning Representations . https://openreview.net/forum?id=d7KBjmI3GmQ.

Huang, Haoyang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei (2023), Not all languages are created equal in LLMs: Improving multilingual capability by crosslingual-thought prompting, in Bouamor, Houda, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , Association for Computational Linguistics, Singapore, pp. 12365-12394. https://aclanthology.org/2023.findings-emnlp.826.

Javaheripi, Mojan, S'ebastien Bubeck, Marah Abdin, Jyoti Aneja, Caio C'esar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang (2023), Phi-2: The surprising power of small language models. https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/ .

Jiang, Albert Q., Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth'ee Lacroix, and William El Sayed (2023), Mistral 7b. https://arxiv.org/abs/2310.06825.

- Joshi, Raviraj, Kanishk Singla, Anusha Kamath, Raunak Kalani, Rakesh Paul, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar, and Eileen Long (2024), Adapting multilingual llms to low-resource languages using continued pre-training and synthetic corpus. https://arxiv.org/abs/2410.14815.
- Lai, Viet, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen (2023), Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback, in Feng, Yansong and Els Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , Association for Computational Linguistics, Singapore, pp. 318-327. https://aclanthology.org/2023.emnlp-demo.28.
- Li, Yuanzhi, S'ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee (2023), Textbooks are all you need ii: phi-1.5 technical report. https://arxiv.org/abs/2309.05463.
- Nguyen, Thuat, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen (2024), CulturaX: A cleaned, enormous, and multilingual dataset for large language models in 167 languages, in Calzolari, Nicoletta, MinYen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , ELRA and ICCL, Torino, Italia, pp. 42264237. https://aclanthology.org/2024.lrec-main.377.

Nielsen, Dan Saattrup (2023), ScandEval: A Benchmark for Scandinavian Natural Language Processing, Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa) , pp. 185-201.

Penedo, Guilherme, Hynek Kydl'ıˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf (2024), Fineweb2: A sparkling update with 1000s of languages. https://huggingface.co/datasets/HuggingFaceFW/fineweb-2.

- Qin, Libo, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che (2023), Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages, in Bouamor, Houda, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Association for Computational Linguistics, Singapore, pp. 2695-2709. https://aclanthology.org/2023.emnlp-main.163.

Qwen Team (2024), Qwen2.5: A party of foundation models. https://qwenlm.github.io/blog/qwen2.5/.

Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn (2024), Direct preference optimization: Your language model is secretly a reward model. https://arxiv.org/abs/2305.18290.

Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu (2020), Exploring the limits of transfer learning with a unified text-to-text transformer, Journal of Machine Learning Research 21 (140), pp. 1-67. http://jmlr.org/papers/v21/20-074.html.

Warstadt, Alex, Amanpreet Singh, and Samuel R. Bowman (2019), Neural network acceptability judgments, Transactions of the Association for Computational Linguistics 7 , pp. 625-641, MIT Press, Cambridge, MA. https://aclanthology.org/Q19-1040.

Willard, Brandon T and R'emi Louf (2023), Efficient guided generation for llms. https://arxiv.org/abs/2307.09702.

Zellers, Rowan, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi (2019), HellaSwag: Can a machine really finish your sentence?, in Korhonen, Anna, David Traum, and Llu'ıs M'arquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , Association for Computational Linguistics, Florence, Italy, pp. 4791-4800. https://aclanthology.org/P19-1472.

Zhang, Chen, Xiao Liu, Jiuheng Lin, and Yansong Feng (2024), Teaching large language models an unseen language on the fly, in Ku, Lun-Wei, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024 , Association for Computational Linguistics, Bangkok, Thailand, pp. 8783-8800. https://aclanthology.org/2024.findingsacl.519.

## Appendix A. Bad word list

This word list contains offensive words! Documents containing any of these words were not included in the final training dataset.

Note that this is a strict list, included ambiguous words such as 'zak' and 'uilskuiken' that also have non-offensive readings. Nevertheless, for the sake of continue-pretraining (rather than fromscratch pretraining) on high quality, it was decided to have a wide coverage of potential bad words. In future versions, this list will be refined and 'bad documents' will be removed with the addition of different metrics rather than only a word list, e.g. by automatic classification or perplexity measures.

```
BAD\_PHRASES\_DOC\_LEVEL = { # https://en.wikipedia.org/wiki/Dutch\_profanity "achterlijk", "debiel", "downie", "idioot", "kankerlijer", "klere", "kolere", "minkukel", "pestkop", "pleuris", "pleuritis", "teringlijer", "tyfuslijer", "gadver", "getver", "godver", "godskolere", "godverork", "graftak", "kopvod", "verdomme", "anaalgeneraal", "bitch", "dikzak",
```

```
"flikker", "fok", "fuck", "hoer", "klootzak", "klote", "kreng", "kringspiermusketier", "kut", "lamzak", "lul", "manwijf", "matennaai", "neuken", "neuker", "ouwehoer", "reet", "reetkever", "reetridder", "rotzak", "schijt", "shit", "slet", "slijmbal", "slons", "sodemieter", "stoephoer", "swaffel", "teef", "trut", "tut", "zak", "uilskuiken", "zeik", "bamivreter", "bosneger", "neger", "fransoos", "geitenneuker", "kaaskop", "kakker", "koelie", "lijp", "medelander", "mocro", "mof", "nikker", "poepchinees", "roetmop", "spaghettivreter", "loempiavouwer", "spanjool", "spleetoog", "tatta", "tokkie",
```

```
"zandneger", "zwartzak", "halvezool", "kenau", "klootviool", "knuppel", "koekert", "koekwaus", "oelewapper", "smeerlap", "sukkel", "sul", "wappie", "wijf", "zooi", # xxx (a.o. https://gitlab.com/yhavinga/c4nlpreproc/-/blob/master/clean/badwords\_ennl.py) "xxx", "anal", "blowjob", "buttplug", "cock", "cunt", "geil", "sex", # Standaardnederlands = seks, maybe we catch some porn or socialmedia sites with this misspelling "porn", # extra "nigger", "nigga", "hoerig", "klojo", }
```

## Appendix B. Training configuration

In all cases, training was run with the alignment-handbook codebase (Tunstall et al. 2024). For the purpose of continued pretraining, I pushed a new task to the alignment-handbook that is now available for anyone who wants to use the codebase for further pretraining. Training runs were run on the Flemish Supercomputer (VSC).

Model creation is fully reproducible thanks to open data and the available configuration files. Below the alignment-handbook config files are given. They can also be found on GitHub. 18

## B.1 Fietje 2B (base model)

```
# Model arguments model\_name\_or\_path: microsoft/phi-2 model\_revision: main torch\_dtype: bfloat16 use\_flash\_attention\_2: true bf16: true tf32: true
```

18. https://github.com/BramVanroy/fietje-2/tree/main/training

```
# Training arguments learning\_rate: 9.0e-05 adam\_beta1: 0.9 adam\_beta2: 0.98 adam\_epsilon: 1.0e-7 weight\_decay: 0.1 logging\_steps: 1 logging\_strategy: steps lr\_scheduler\_type: linear max\_seq\_length: 2048 per\_device\_train\_batch\_size: 40 per\_device\_eval\_batch\_size: 40 gradient\_accumulation\_steps: 3 gradient\_checkpointing: true gradient\_checkpointing\_kwargs: use\_reentrant: False # Data training arguments dataset\_mixer: /dodrio/scratch/projects/2023\_071/alignment-handbook/data/fietje-2b-cpt-prep: 1.0 dataset\_splits: - train - test preprocessing\_num\_workers: 8 num\_train\_epochs: 1.0 remove\_unused\_columns: true push\_to\_hub: true report\_to: - wandb log\_level: info # To do or not to do do\_train: True do\_eval: True seed: 42 # Storing output\_dir: /dodrio/scratch/projects/2023\_071/alignment-handbook/data/fietje-2b overwrite\_output\_dir: true save\_total\_limit: 6 hub\_model\_id: fietje-2b hub\_private\_repo: true hub\_strategy: all\_checkpoints # Strategies evaluation\_strategy: "steps" eval\_steps: 900 save\_strategy: "steps" save\_steps: 900 warmup\_steps: 0
```

## B.2 Fietje 2B Instruct (instruction model)

```
# Model arguments model\_name\_or\_path: BramVanroy/fietje-2b model\_revision: main torch\_dtype: bfloat16 use\_flash\_attention\_2: true bf16: true tf32: true # Training arguments learning\_rate: 6.0e-05 adam\_beta1: 0.9 adam\_beta2: 0.98 adam\_epsilon: 1.0e-7 weight\_decay: 0.1 logging\_steps: 1 logging\_strategy: steps lr\_scheduler\_type: cosine max\_seq\_length: 2048 per\_device\_train\_batch\_size: 42 per\_device\_eval\_batch\_size: 36 gradient\_accumulation\_steps: 1 gradient\_checkpointing: true gradient\_checkpointing\_kwargs: use\_reentrant: False # Data training arguments chat\_template: "{% for message in messages %}{{'<|im\_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add\_generation\_prompt) or not loop.last %}{{ '<|im\_end|>' + '\n'}}{% endif %}{% endfor %}{% if add\_generation\_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im\_start|>assistant\n' }}{% endif %}" dataset\_mixer: BramVanroy/ultrachat\_200k\_dutch: 1.0 BramVanroy/no\_robots\_dutch: 1.0 BramVanroy/belebele\_dutch: 1.0 dataset\_configs: - default - default - sft dataset\_splits: - train\_sft - test\_sft preprocessing\_num\_workers: 8 num\_train\_epochs: 3.0 remove\_unused\_columns: true push\_to\_hub: true report\_to: - wandb log\_level: info # To do or not to do do\_train: True
```

```
do\_eval: True seed: 42 # Storing output\_dir: /dodrio/scratch/projects/2023\_071/alignment-handbook/data/fietje-2b-sft overwrite\_output\_dir: true hub\_model\_id: BramVanroy/fietje-2b-sft hub\_private\_repo: true hub\_strategy: all\_checkpoints save\_total\_limit: 6 # Strategies evaluation\_strategy: "epoch" save\_strategy: "epoch" warmup\_ratio: 0.1
```

## B.3 Fietje 2B Chat (preference model)

```
# Model arguments model\_name\_or\_path: BramVanroy/fietje-2b-sft model\_revision: main torch\_dtype: bfloat16 use\_flash\_attention\_2: true bf16: true tf32: true # Training arguments learning\_rate: 2.0e-06 adam\_beta1: 0.9 adam\_beta2: 0.98 adam\_epsilon: 1.0e-7 weight\_decay: 0.1 logging\_steps: 1 logging\_strategy: steps lr\_scheduler\_type: cosine max\_length: 2048 max\_prompt\_length: 1280 # DPO beta: 0.2 per\_device\_train\_batch\_size: 8 per\_device\_eval\_batch\_size: 4 gradient\_accumulation\_steps: 2 gradient\_checkpointing: true gradient\_checkpointing\_kwargs: use\_reentrant: False # Data training arguments dataset\_mixer: BramVanroy/ultra\_feedback\_dutch\_cleaned: 1.0 BramVanroy/orca\_dpo\_pairs\_dutch\_cleaned: 1.0 dataset\_configs:
```

```
- dpo\_hq - dpo\_all dataset\_splits: - train\_prefs - test\_prefs preprocessing\_num\_workers: 8 num\_train\_epochs: 1.0 remove\_unused\_columns: true push\_to\_hub: true report\_to: - wandb log\_level: info # To do or not to do do\_train: True do\_eval: True seed: 42 # Storing output\_dir: /dodrio/scratch/projects/2023\_071/alignment-handbook/data/fietje-2b-dpo overwrite\_output\_dir: true hub\_model\_id: BramVanroy/fietje-2b-dpo hub\_private\_repo: true hub\_strategy: all\_checkpoints save\_total\_limit: 6 # Strategies evaluation\_strategy: "epoch" save\_strategy: "epoch" warmup\_ratio: 0.1
```

## Appendix C. Benchmark templates

These prompt templates are also available in the config files of the benchmarks at https://github. com/BramVanroy/clin34-benchmarks/tree/main/configs .

## C.1 ARC

For models without a chat template ('base' models), the text 'Het antwoord is ' is added to the end of the prompt.

```
{{- instruction }} {% set options = [ ('A', option\_a), ('B', option\_b), ('C', option\_c), ('D', option\_d) ] -%} {%- set available\_options = options | selectattr('1', 'defined') | rejectattr('1', 'none') | list -%} {%- if available\_options -%} Antwoordopties: {%- for letter, option in available\_options %}
```

```
{{ letter }}. {{ option }} {%- endfor %} Antwoord met {% for i in range(available\_options|length) -%} '{{ available\_options[i][0] }}'{% if i + 2 == available\_options|length %} of {% elif i + 1 < available\_options|length %}, {% endif %} {%- endfor -%}. {%- endif -%}
```

## C.2 DBRD

For models without a chat template ('base' models), the text 'Het sentiment is ' is added to the end of the prompt.

```
Is het sentiment in de volgende Nederlandstalige boekrecensie positief of negatief? Boekrecensie: {{ text }} Antwoord met 'positief' of 'negatief'.
```

## C.3 Dutch CoLA

For models without a chat template ('base' models), the text 'De tekst is ' is added to the end of the prompt.

```
Is de volgende tekst grammaticaal (correct Nederlands) of ongrammaticaal (onjuist Nederlands)? Tekst: {{ Sentence }} Antwoord met 'grammaticaal' of 'ongrammaticaal'.
```

## C.4 Global MMLU

For models without a chat template ('base' models), the text 'Het antwoord is ' is added to the end of the prompt.

```
{{- question }} {% set options = [ ('A', option\_a), ('B', option\_b), ('C', option\_c), ('D', option\_d) ] -%} {%- set available\_options = options | selectattr('1', 'defined') | rejectattr('1', 'none') | list -%} {%- if available\_options -%} Antwoordopties: {%- for letter, option in available\_options %} {{ letter }}. {{ option }} {%- endfor %} Antwoord met {% for i in range(available\_options|length) -%}
```

```
'{{ available\_options[i][0] }}'{% if i + 2 == available\_options|length %} of {% elif i + 1 < available\_options|length %}, {% endif %} {%- endfor -%}. {%- endif -%}
```

```
C.5 XLWIC target word
```

For models without a chat template ('base' models), the text 'De betekenis van ' {{ }} ' is ' is added to the end of the prompt.

```
Is de betekenis van '{{ target\_word }}' in de volgende zinnen identiek of verschillend? Zin 1: {{ example\_1 }} Zin 2: {{ example\_2 }} Antwoord met 'identiek' of 'verschillend'.
```

## Appendix D. Benchmark visualizations

```
D.1 Model performance vs. model size (per task) D.2 Model performance vs. model size (per task)
```

(a) ARC

<!-- image -->

<!-- image -->

(c) Dutch CoLA

(b) DBRD

<!-- image -->

(d) Global-MMLU

<!-- image -->

Figure 4: Performance vs. size across all benchmarks

<!-- image -->

(a) ARC

<!-- image -->

<!-- image -->

(c) Dutch CoLA

(b) DBRD

<!-- image -->

(d) Global-MMLU

<!-- image -->

Figure 5: Performance vs. release date across all benchmarks

<!-- image -->